{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa79d934-8c02-49f4-b189-d9cbd9f6e500",
   "metadata": {},
   "source": [
    "# GloVe Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7964c395-6162-436f-b41c-5bb28f2b6074",
   "metadata": {},
   "source": [
    "<b>Name:</b> Min Set Aung <b>Student Id:</b> st122825"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b269f6c-9a89-48e1-bc49-b74601ca4acc",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e030e2e-8e9c-444f-9078-f3d147788d0d",
   "metadata": {},
   "source": [
    "In this assignment, the implemented embedding models (Skip-gram, Skip-gram with negative sampling, CBOW, and GloVe) were trained and compared based on their syntactic and semantic accuracy. Word-analogy task dataset was used for the assessment. Brown corpus was used for training the models. To reduce the number of vocabularies due to memeory limitation, only news category files were used. Then the first 2000 sentences were used from the  corpus category to further reduce the vocabulary size. For the models to train more more effectively, stopwords (commonly used words like \"the\", \"a\", and \"an\") and punctuations are filtered out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5435d9a3-a239-489d-8404-07105f810bd9",
   "metadata": {},
   "source": [
    "Also, another assessment was performed using a similarity dataset (WordSim353). WordSim353 contains pairs of words and a score is given by a human judge on how similar/related the pair are. In this assessment, only the similarity part is checked. The main purpose of the evaluation is to check the correlation of the similarity given by the trained models with the similarity score given by the judge for word pairs similarity. Spearman correlation coefficient with associated p-value is used as a correlation metric for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b8a17f4-df3b-4f15-8ccf-da33b36c3702",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0e2522-650e-4e5f-8c3a-dd0f81d203ef",
   "metadata": {},
   "source": [
    "## 1) Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6edb3205-5f9a-4051-b9da-5ebc24acbc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bf489a6-52e6-4935-9f7c-f1431ca5a3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25e990db-089b-4c72-904d-ccf017527b64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ccbd97-81b7-4aea-9e60-3e8b631241ae",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/15547409/how-to-get-rid-of-punctuation-using-nltk-tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14ac0fe4-4b83-460a-bd82-7b0d65275b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed07caf9-d31d-443e-a25d-361a563c634f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d038caf5-0d30-49d3-b5d0-2494c32109d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = []\n",
    "for i in string.punctuation:\n",
    "    punctuations.append(i)\n",
    "punctuations.append(\"''\")\n",
    "punctuations.append('\"\"')\n",
    "punctuations.append('``')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ec5561-b5c7-4c65-b6b8-f50fd3a1502f",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/removing-stop-words-nltk-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c5edef3-22b6-405d-9db1-c09817e3e3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a6e985e-d302-4047-8c99-446a83db782c",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b60312bb-0ffd-4583-8570-556737c0babe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a1dd758-d59e-4376-8b8b-0911f967917e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05265fd1-51d8-4959-9e20-21da1cc8f653",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized = []\n",
    "sentence = []\n",
    "for word in brown.words(categories='news'):\n",
    "    if word == \".\":\n",
    "        corpus_tokenized.append(sentence)\n",
    "        sentence = []\n",
    "    elif word in punctuations or word.lower() in stop_words:\n",
    "        continue\n",
    "    sentence.append(word.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5e58809-5c85-4c40-ae52-fec274808b7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4030"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d7a9f7db-2ec2-49dc-ad91-4839f06b5bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokenized = corpus_tokenized[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3333e521-542f-4dd3-a12a-4ca192ea1ee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fulton',\n",
       " 'county',\n",
       " 'grand',\n",
       " 'jury',\n",
       " 'said',\n",
       " 'friday',\n",
       " 'investigation',\n",
       " \"atlanta's\",\n",
       " 'recent',\n",
       " 'primary',\n",
       " 'election',\n",
       " 'produced',\n",
       " 'evidence',\n",
       " 'irregularities',\n",
       " 'took',\n",
       " 'place']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tokenized[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27730a4e-bbb9-4719-a4f2-5700054be202",
   "metadata": {},
   "source": [
    "## 2) Prepare Data for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc77af98-213c-430f-a819-0d5a703cc11d",
   "metadata": {},
   "source": [
    "### Creating vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f0c093a1-8093-4e28-ad0a-22eee50e6c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we want to flatten this (basically merge all list)\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "vocabs  = list(set(flatten(corpus_tokenized)))  #vocabs is a term defining all unique words your system know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "409bcfd3-f344-41cd-9153-735e438e75dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"kowalski's\", '44', 'bay-front', 'detriment', '8-4', 'containing', '4-homer', 'fumble', 'next', '6.5']\n"
     ]
    }
   ],
   "source": [
    "print(vocabs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b8915029-ef92-4043-9e98-9c4340c1f60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 8029\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size:\", len(vocabs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62d613d-5991-490b-a89b-d28c4c8c1ff9",
   "metadata": {},
   "source": [
    "### Numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a7cabe9-df19-40ac-99d9-2f9121d1b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = {v: idx for idx, v in enumerate(vocabs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80758c68-7eac-4053-b425-b4c0797ea4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add <UNK>, which is a very normal token exists in the world\n",
    "vocabs.append('<UNK>') #chaky, can it be ##UNK, or UNKKKKKK, or anything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26509dce-4bbb-4bbe-b958-4a88a7cb1dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we have a way to know what is the id of <UNK>\n",
    "word2index['<UNK>'] = 6  #usually <UNK> is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6ae9f5e9-771d-43bc-8b9a-a4de7b868aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create index2word dictionary\n",
    "#2 min    \n",
    "index2word = {v:k for k, v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2be3a6d7-d0d1-4b2a-8ee0-9e0ea0444ba8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4689"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2index[\"greece\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "701b1e82-007f-45cc-a60c-439e3c106faa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$4'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2word[7974]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae24f919-b9a1-426e-b03e-0159a6395722",
   "metadata": {},
   "source": [
    "### Creating (context word, outside word) tuples for Skip-gram, Skip-gram with negative sampling, GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4b50df1-5b3c-4b15-be00-058e0b28587c",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "85dcbf98-af5b-43b2-bae9-61f24a6fd5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4235, 472],\n",
       " [4235, 4121],\n",
       " [4235, 4034],\n",
       " [4235, 5184],\n",
       " [4121, 4235],\n",
       " [4121, 5184],\n",
       " [4121, 472],\n",
       " [4121, 4378],\n",
       " [5184, 4121],\n",
       " [5184, 4378]]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgrams = []\n",
    "\n",
    "#for each corpus\n",
    "for sent in corpus_tokenized:\n",
    "    #for each sent\n",
    "    for i in range(window_size, len(sent) - window_size):\n",
    "        center_word = word2index[sent[i]]\n",
    "        outside_words = []\n",
    "        for j in range(1, window_size + 1):\n",
    "            outside_words.append(word2index[sent[i-j]])\n",
    "            outside_words.append(word2index[sent[i+j]])\n",
    "            \n",
    "        for o in outside_words:\n",
    "            skipgrams.append([center_word, o])\n",
    "\n",
    "skipgrams[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf2a1c1-263d-4fae-861a-4f8da15126ea",
   "metadata": {},
   "source": [
    "### Unigram distribution\n",
    "$$P(w)=U(w)^{3/4}/Z$$\n",
    "\n",
    "Defining the probability of sampling negative words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2dedc033-3002-4770-8b8f-94d7d19f7636",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8f6fe5f-aacc-4f22-990a-e56eb5c19568",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e13d5e69-3622-4958-9d67-b766498589b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter(flatten(corpus_tokenized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1902af49-de25-4920-b7a0-fdc9f62434dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28276"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_total_words = sum([c for w, c in word_count.items()])\n",
    "num_total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b2a8c33e-7b57-4e18-ba8b-c752278fba3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_table = []\n",
    "\n",
    "for v in vocabs:\n",
    "    uw = word_count[v]/num_total_words\n",
    "    uw_alpha = uw ** 0.75\n",
    "    uw_alpha_dividebyz = int(uw_alpha / z)\n",
    "    # print(\"vocab: \", v)\n",
    "    # print(\"distribution: \", uw_alpha_dividebyz)\n",
    "    unigram_table.extend([v] * uw_alpha_dividebyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a82fa9d2-f27d-4c1d-b29c-767e32b60407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['containing', 'next', 'next', 'next', 'next']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_table[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e48c0c-90e5-410c-82a6-5b1df9ea4206",
   "metadata": {},
   "source": [
    "### Creating (context word, outside word) tuples for CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8138869-5200-4a7b-872c-266e1f55606e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4235, [4034, 472, 4121, 5184]],\n",
       " [4121, [472, 4235, 5184, 4378]],\n",
       " [5184, [4235, 4121, 4378, 4358]],\n",
       " [4378, [4121, 5184, 4358, 1611]],\n",
       " [4358, [5184, 4378, 1611, 4794]],\n",
       " [1611, [4378, 4358, 4794, 3043]],\n",
       " [4794, [4358, 1611, 3043, 6551]],\n",
       " [3043, [1611, 4794, 6551, 602]],\n",
       " [6551, [4794, 3043, 602, 1470]],\n",
       " [602, [3043, 6551, 1470, 5900]]]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skipgrams_CBOW = []\n",
    "\n",
    "#for each corpus\n",
    "for sent in corpus_tokenized:\n",
    "    for i in range(window_size, len(sent) - window_size):\n",
    "        center_word   = word2index[sent[i]]\n",
    "        outside_words = []\n",
    "        \n",
    "        low  = i - window_size\n",
    "        high = i + window_size\n",
    "        for j in range(low, high + 1):\n",
    "            if j == i:\n",
    "                continue\n",
    "            outside_words.append(word2index[sent[j]])\n",
    "        skipgrams_CBOW.append([center_word, outside_words])\n",
    "\n",
    "skipgrams_CBOW[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7a0310-f946-4719-8172-a9ea33dc0d81",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Co-occurrence matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a8fab8eb-a0c5-4391-a00a-f4d0b2015c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the frequency of each word....\n",
    "X_i = Counter(flatten(corpus_tokenized)) #merge all list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9c5a1a2-13cd-43a9-a2b8-2478a055044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gloVes = []\n",
    "\n",
    "#loop through each corpus\n",
    "for sent in corpus_tokenized: \n",
    "    #loop through each word from 1 to n-1 (because 0 and n has no context window)\n",
    "    for i in range(1, len(sent)-1):\n",
    "        target  = sent[i]\n",
    "        context = [sent[i+1], sent[i-1]]\n",
    "        #append(i, i+1) and append(i, i-1)\n",
    "        for c in context:\n",
    "            gloVes.append((target, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5060cace-14e6-47f0-a571-b13717ac2f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "gloVe_id = [(word2index[gloVe[0]], word2index[gloVe[1]]) for gloVe in gloVes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39b3aec9-9f97-4431-8ddb-74282528d5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(472, 4235), (472, 4034), (4235, 4121), (4235, 472), (4121, 5184)]\n"
     ]
    }
   ],
   "source": [
    "print(gloVe_id[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a16c951-bf2c-4a9a-bd5c-9ba154272fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since we have these occurrences, we can count, to make our co-occurrence matrix!!!\n",
    "X_ik_skipgram = Counter(gloVes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27e2adf2-da6f-4473-8cee-c123fe15ea27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_ik_skipgram[(\"fulton\", \"county\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a533102-c09d-46f5-b82d-7f45bd70c73f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Weighting function f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "102078d2-510d-4465-94e4-cdb3cee61915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighting(w_i, w_j, X_ik):   #why we need w_i and w_j, because we can try its co-occurrences, if it's too big, we scale it down\n",
    "    \n",
    "    #check whether the co-occurrences between these two word exists???\n",
    "    try:\n",
    "        x_ij = X_ik[(w_i, w_j)]\n",
    "    except:\n",
    "        x_ij = 1  #why one, so that the probability thingy won't break...(label smoothing)\n",
    "        \n",
    "    #maximum co-occurrences; we follow the paper\n",
    "    x_max = 100\n",
    "    alpha = 0.75\n",
    "    \n",
    "    #if the co-occurrences does not exceed x_max, scale it down based on some alpha\n",
    "    if x_ij < x_max:\n",
    "        result = (x_ij/x_max) ** alpha\n",
    "    else:\n",
    "        result = 1 #this is the maximum probability you can have\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c38fa7f1-4c40-4430-a3f8-7448978ee4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10573712634405642\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "w_i  = 'fulton'\n",
    "w_j  = 'county'\n",
    "w_j2 = 'chaky'\n",
    "\n",
    "print(weighting(w_i, w_j, X_ik_skipgram))   #scales from 1 to 0.0316\n",
    "print(weighting(w_i, w_j2, X_ik_skipgram))  #the paper says that f(0) = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8d6c62f1-c969-4fa6-a025-67901dcd4432",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now apply this weighting to alcorpus_tokenizedible pairs\n",
    "from itertools import combinations_with_replacement\n",
    "\n",
    "X_ik = {} #for keeping the co-occurrences\n",
    "weighting_dic = {} #for keeping all the probability after passing through the weighting function\n",
    "\n",
    "for bigram in combinations_with_replacement(vocabs, 2):  #we need to also think its reverse\n",
    "    #if this bigram exists in X_ik_skipgrams\n",
    "    #we gonna add this to our co-occurence matrix\n",
    "    if X_ik_skipgram.get(bigram) is not None:\n",
    "        cooc = X_ik_skipgram[bigram]  #get the co-occurrence\n",
    "        X_ik[bigram] = cooc + 1 #this is again basically label smoothing....(stability issues (especially when divide something))\n",
    "        X_ik[(bigram[1], bigram[0])] = cooc + 1  #trick to get all pairs\n",
    "    else: #otherwise, do nothing\n",
    "        pass\n",
    "    \n",
    "    #apply the weighting function using this co-occurrence matrix thingy    \n",
    "    weighting_dic[bigram] = weighting(bigram[0], bigram[1], X_ik)\n",
    "    weighting_dic[(bigram[1], bigram[0])] = weighting(bigram[1], bigram[0], X_ik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "439da6eb-eb94-46d4-8cf9-58d1634f5cac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43854"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_ik_skipgram)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bb9aad-0b63-4c82-b85e-d4ee969e14b1",
   "metadata": {},
   "source": [
    "### Create random batch sampler function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3fa69015-67ec-4a2f-a309-1a2ab7ae927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch(batch_size, corpus, skip_grams):\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(skip_grams)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([skip_grams[i][1]])  # context word, e.g., 3\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec18c7-30e5-4a38-be7a-3f4696312c1a",
   "metadata": {},
   "source": [
    "<b>Test Function</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "69469e8e-78cd-4eb2-ba83-22255c07d553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sample: [[4896]\n",
      " [  76]\n",
      " [1244]\n",
      " [1952]\n",
      " [7725]]\n",
      "Input shape: (5, 1)\n",
      "\n",
      "Label sample: label=array([[7347],\n",
      "       [3483],\n",
      "       [3165],\n",
      "       [5455],\n",
      "       [5677]])\n",
      "Label shape: (5, 1)\n"
     ]
    }
   ],
   "source": [
    "input, label = random_batch(5, corpus_tokenized, skipgrams)\n",
    "\n",
    "print(\"Input sample:\", input)\n",
    "print(f\"Input shape: {input.shape}\", end=\"\\n\\n\")\n",
    "print(f\"Label sample: {label=}\")\n",
    "print(\"Label shape:\", label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7033c95-727f-45ec-8972-a26cf474af3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ff1c83b6-469e-4ce1-a838-24323ee7c302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_batch_cbow(batch_size, corpus, cbow):\n",
    "    random_inputs = []\n",
    "    random_labels = []\n",
    "    random_index = np.random.choice(range(len(cbow)), batch_size, replace=False) #randomly pick without replacement\n",
    "        \n",
    "    for i in random_index:\n",
    "        random_inputs.append([cbow[i][0]])  # target, e.g., 2\n",
    "        random_labels.append([cbow[i][1]])  # context word, e.g., 3\n",
    "            \n",
    "    return np.array(random_inputs), np.array(random_labels).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ade3116-fc18-44f8-bf1b-cd0b4062dfa8",
   "metadata": {},
   "source": [
    "<b>Test Function</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6c1ccefd-9564-4550-b288-d3c1234fbac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sample: [[  30]\n",
      " [7674]\n",
      " [7398]\n",
      " [ 518]\n",
      " [5912]]\n",
      "Input shape: (5, 1)\n",
      "\n",
      "Label sample: label=array([[7261, 6148, 7661, 7638],\n",
      "       [7212, 2729, 2250,   48],\n",
      "       [4410, 5184, 4842, 3483],\n",
      "       [6288, 3106, 7690, 4601],\n",
      "       [7017, 4456, 3944, 2597]])\n",
      "Label shape: (5, 4)\n"
     ]
    }
   ],
   "source": [
    "input, label = random_batch_cbow(5, corpus_tokenized, skipgrams_CBOW)\n",
    "\n",
    "print(\"Input sample:\", input)\n",
    "print(f\"Input shape: {input.shape}\", end=\"\\n\\n\")\n",
    "print(f\"Label sample: {label=}\")\n",
    "print(\"Label shape:\", label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f54f405-6d3a-4dbe-a749-17c93cf4939b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14357b47-095d-406b-bec5-cd161490b871",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def random_batch_gloVe(batch_size, word_sequence, skip_grams, X_ik, weighting_dic):\n",
    "    \n",
    "    #loop through this skipgram, and change it id  because when sending model, it must number\n",
    "    skip_grams_id = [(word2index[skip_gram[0]], word2index[skip_gram[1]]) for skip_gram in skip_grams]\n",
    "    \n",
    "    #randomly pick \"batch_size\" indexes\n",
    "    number_of_choices = len(skip_grams_id)\n",
    "    random_index = np.random.choice(number_of_choices, batch_size, replace=False) #no repeating indexes among these random indexes\n",
    "    \n",
    "    random_inputs = [] #xi, wi (in batches)\n",
    "    random_labels = [] #xj, wj (in batches)\n",
    "    random_coocs  = [] #Xij (in batches)\n",
    "    random_weighting = [] #f(Xij) (in batches)\n",
    "    #for each of the sample in these indexes\n",
    "    for i in random_index:\n",
    "        random_inputs.append([skip_grams_id[i][0]]) #same reason why i put bracket here....\n",
    "        random_labels.append([skip_grams_id[i][1]])\n",
    "        \n",
    "        #get cooc\n",
    "        #first check whether it exists...\n",
    "        pair = skip_grams[i]  #e.g., ('banana', 'fruit)\n",
    "        try:\n",
    "            cooc = X_ik[pair]\n",
    "        except:\n",
    "            cooc = 1 #label smoothing\n",
    "            \n",
    "        random_coocs.append([math.log(cooc)])  #1. why log, #2, why bracket -> size ==> (, 1)  #my neural network expects (, 1)\n",
    "        \n",
    "        #get weighting\n",
    "        weighting = weighting_dic[pair]  #why not use try....maybe it does not exist....\n",
    "        random_weighting.append(weighting)\n",
    "\n",
    "        \n",
    "    return np.array(random_inputs), np.array(random_labels), np.array(random_coocs), np.array(random_weighting)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a97fea7-d989-4a6b-96f2-3b04759a0d0e",
   "metadata": {},
   "source": [
    "<b>Test Function</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7cd5e928-7869-4ef9-b581-01dbb109d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "input, target, cooc, weightin = random_batch_gloVe(5, corpus_tokenized, gloVes, X_ik, weighting_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1333966-9e6d-4f35-8d7a-effb0f0221e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sample: [[1896]\n",
      " [7092]\n",
      " [7166]\n",
      " [5666]\n",
      " [3683]]\n",
      "Input shape: (5, 1)\n",
      "\n",
      "Label sample: label=array([[7261, 6148, 7661, 7638],\n",
      "       [7212, 2729, 2250,   48],\n",
      "       [4410, 5184, 4842, 3483],\n",
      "       [6288, 3106, 7690, 4601],\n",
      "       [7017, 4456, 3944, 2597]])\n",
      "Label shape: (5, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Input sample:\", input)\n",
    "print(f\"Input shape: {input.shape}\", end=\"\\n\\n\")\n",
    "print(f\"Label sample: {label=}\")\n",
    "print(\"Label shape:\", label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "73d362cb-6fdf-4266-9fc6-c45e13ccce78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1896],\n",
       "        [7092],\n",
       "        [7166],\n",
       "        [5666],\n",
       "        [3683]]),\n",
       " array([[6780],\n",
       "        [6060],\n",
       "        [5134],\n",
       "        [4922],\n",
       "        [ 405]]),\n",
       " array([[0.69314718],\n",
       "        [0.69314718],\n",
       "        [0.69314718],\n",
       "        [2.30258509],\n",
       "        [0.69314718]]),\n",
       " array([0.05318296, 0.05318296, 0.05318296, 0.17782794, 0.05318296]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, target, cooc, weightin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db829659-5cc3-4db6-a9ad-99762a68e48a",
   "metadata": {},
   "source": [
    "### Helper functions for skipgram with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ddd114ed-4656-4901-912c-1fb4ad59aa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, word2index):\n",
    "    #map(function, list of something)\n",
    "    #map will look at each of element in this list, and apply this function\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ca930f7a-9957-46d7-9181-0879e61daae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "#you don't want to pick samples = targets, basically negative samples\n",
    "#k = number of negative samples - how many? they found 10 is the best\n",
    "#will be run during training\n",
    "#after random_batch, \n",
    "def negative_sampling(targets, unigram_table, k):\n",
    "    #targets is already in id.....\n",
    "    #but the unigram_table is in word....\n",
    "    #1. get the batch size of this targets\n",
    "    batch_size = targets.shape[0]\n",
    "    neg_samples = []\n",
    "    #2. for each batch\n",
    "    for i in range(batch_size):\n",
    "        #randomly pick k negative words from unigram_table\n",
    "        target_index = targets[i].item()  #looping each of the batch....\n",
    "        nsample = []\n",
    "        while len(nsample) < k:\n",
    "            neg = random.choice(unigram_table)\n",
    "            #if this word == target, skip this word\n",
    "            if word2index[neg] == target_index:\n",
    "                continue\n",
    "            nsample.append(neg)\n",
    "        #append this word to some list\n",
    "        neg_samples.append(prepare_sequence(nsample, word2index).reshape(1, -1))  #tensor[], tensor[]\n",
    "    return torch.cat(neg_samples)  #tensor[[], []]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70824e1-7400-46d4-8e36-e8e9521e4a26",
   "metadata": {},
   "source": [
    "<b>Test Functions</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "63d5b9f6-488b-48df-95a4-f38e98d56316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[4358],\n",
       "        [7212]]),\n",
       " array([[2375],\n",
       "        [6786]]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_batch, label_batch = random_batch(2, corpus_tokenized, skipgrams)\n",
    "\n",
    "input_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f45baca8-112e-4b44-b834-2496481b98b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch = torch.LongTensor(input_batch)\n",
    "label_batch = torch.LongTensor(label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "da6ba236-6318-4a79-862c-fb680bb7e1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neg = 5  #in the real code, we gonna use 10 (like in the paper)\n",
    "neg_samples = negative_sampling(label_batch, unigram_table, num_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "35f73271-eead-416a-ba07-77a3e47cb8c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17336053-1b16-4c65-9240-acff3fb640fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89e8d3f4-3cc8-4609-ba7c-b746a16dd0b8",
   "metadata": {},
   "source": [
    "## 3) Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca1a44-fe4c-41f4-a324-ea227882a7e3",
   "metadata": {},
   "source": [
    "### Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1aa124-534e-4d09-a065-71a7dd46e8a4",
   "metadata": {},
   "source": [
    "$$J(\\theta) = -\\frac{1}{T}\\sum_{t=1}^{T}\\sum_{\\substack{-m \\leq j \\leq m \\\\ j \\neq 0}}\\log P(w_{t+j} | w_t; \\theta)$$\n",
    "\n",
    "where $P(w_{t+j} | w_t; \\theta) = $\n",
    "\n",
    "$$P(o|c)=\\frac{\\exp(\\mathbf{u_o^{\\top}v_c})}{\\sum_{w=1}^V\\exp(\\mathbf{u_w^{\\top}v_c})}$$\n",
    "\n",
    "where $o$ is the outside words and $c$ is the center word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8c1c207a-90db-4545-966b-734ac8d1bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model will accept three vectors - u_o, v_c, u_w\n",
    "#u_o - vector for outside words\n",
    "#v_c - vector for center word\n",
    "#u_w - vectors of all vocabs\n",
    "\n",
    "class Skipgram(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(Skipgram, self).__init__()\n",
    "        self.embedding_center_word  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside_word = nn.Embedding(voc_size, emb_size)\n",
    "    \n",
    "    def forward(self, center_word, outside_word, all_vocabs):\n",
    "        #center_word, outside_word: (batch_size, 1)\n",
    "        #all_vocabs: (batch_size, voc_size)\n",
    "        \n",
    "        #convert them into embedding\n",
    "        center_word_embed  = self.embedding_center_word(center_word)     #(batch_size, 1, emb_size)\n",
    "        outside_word_embed = self.embedding_outside_word(outside_word)   #(batch_size, 1, emb_size)\n",
    "        all_vocabs_embed   = self.embedding_outside_word(all_vocabs)     #(batch_size, voc_size, emb_size)\n",
    "        \n",
    "        #bmm is basically @ or .dot , but across batches (i.e., ignore the batch dimension)\n",
    "        top_term = outside_word_embed.bmm(center_word_embed.transpose(1, 2)).squeeze(2)\n",
    "        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) ===> (batch_size, 1)\n",
    "        \n",
    "        top_term_exp = torch.exp(top_term)  #exp(uo vc)\n",
    "        #(batch_size, 1)\n",
    "        \n",
    "        lower_term = all_vocabs_embed.bmm(center_word_embed.transpose(1, 2)).squeeze(2)\n",
    "         #(batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size, 1) = (batch_size, voc_size)\n",
    "         \n",
    "        lower_term_sum = torch.sum(torch.exp(lower_term), 1) #sum exp(uw vc)\n",
    "        #(batch_size, 1)\n",
    "        \n",
    "        loss_fn = -torch.mean(torch.log(top_term_exp / lower_term_sum))\n",
    "        #(batch_size, 1) / (batch_size, 1) ==mean==> scalar\n",
    "        \n",
    "        return loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d9a5ea3e-3215-4048-a261-2bbb74a8495e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 8030])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#preparing all_vocabs\n",
    "\n",
    "batch_size = 5\n",
    "\n",
    "voc_size = len(vocabs)\n",
    "voc_size\n",
    "\n",
    "def prepare_sequence(seq, word2index):\n",
    "    #map(function, list of something)\n",
    "    #map will look at each of element in this list, and apply this function\n",
    "    idxs = list(map(lambda w: word2index[w] if word2index.get(w) is not None else word2index[\"<UNK>\"], seq))\n",
    "    return torch.LongTensor(idxs)\n",
    "\n",
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "all_vocabs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e4970a-db4d-4674-90fb-afce38e2d5f6",
   "metadata": {},
   "source": [
    "<b>Test Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "16e4e356-f02e-4018-b26d-805c51380988",
   "metadata": {},
   "outputs": [],
   "source": [
    "input, label = random_batch(batch_size, corpus_tokenized, skipgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6612c5a8-5de6-4256-96e6-bdab980dabdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[7766],\n",
       "        [3372],\n",
       "        [7626],\n",
       "        [5870],\n",
       "        [4034]]),\n",
       " array([[2758],\n",
       "        [5885],\n",
       "        [6506],\n",
       "        [3256],\n",
       "        [5865]]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "67772a8c-e1de-43a0-80f4-c3487a214942",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 2\n",
    "model = Skipgram(voc_size, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ba2a797d-0341-42d2-9ea6-9edd2900d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.LongTensor(input)  \n",
    "label_tensor = torch.LongTensor(label) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "7946c55a-b2d5-45aa-b5ca-4c0320a588b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(input_tensor, label_tensor, all_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4565cdb9-791d-4b68-bbe1-13e40e555f64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.6690, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d258b61-39b8-4736-b8d3-e507e9c62026",
   "metadata": {},
   "source": [
    "### Skip-gram with negative sampling\n",
    "\n",
    "$$\\mathbf{J}_{\\text{neg-sample}}(\\mathbf{v}_c,o,\\mathbf{U})=-\\log(\\sigma(\\mathbf{u}_o^T\\mathbf{v}_c))-\\sum_{k=1}^K\\log(\\sigma(-\\mathbf{u}_k^T\\mathbf{v}_c))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "15b1c0d4-b1a2-401e-8ae9-1cf68e9b1a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipgramNeg(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(SkipgramNeg, self).__init__()\n",
    "        self.embedding_center_word  = nn.Embedding(voc_size, emb_size)\n",
    "        self.embedding_outside_word = nn.Embedding(voc_size, emb_size)\n",
    "        self.logsigmoid = nn.LogSigmoid()\n",
    "        \n",
    "    def forward(self, center_words, outside_words, negative_words):\n",
    "        #center_words, outside_words: (batch_size, 1)\n",
    "        #negative_words:  (batch_size, k)\n",
    "        \n",
    "        center_embed  = self.embedding_center_word(center_words)    #(batch_size, 1, emb_size)\n",
    "        outside_embed = self.embedding_outside_word(outside_words)  #(batch_size, 1, emb_size)\n",
    "        neg_embed     = self.embedding_outside_word(negative_words) #(batch_size, k, emb_size)\n",
    "        \n",
    "        uovc          =  outside_embed.bmm(center_embed.transpose(1, 2)).squeeze(2)  #(batch_size, 1)\n",
    "        ukvc          = -neg_embed.bmm(center_embed.transpose(1, 2)).squeeze(2)  #(batch_size, k)\n",
    "        ukvc_sum      =  torch.sum(ukvc, 1).view(-1, 1) #(batch_size, 1)\n",
    "        \n",
    "        loss = self.logsigmoid(uovc) + self.logsigmoid(ukvc_sum)  #(batch_size, 1) + (batch_size, 1)\n",
    "                \n",
    "        return -torch.mean(loss)  #scalar, loss should be scalar, to call backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a2b00-8c4a-4c1b-9a8a-adbaf084dc7e",
   "metadata": {},
   "source": [
    "<b>Test Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7051430e-62da-4354-99ce-22c1a5d0fb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input, label = random_batch(batch_size, corpus_tokenized, skipgrams)\n",
    "input_tensor = torch.LongTensor(input)  \n",
    "label_tensor = torch.LongTensor(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "00ee210d-62b8-441a-a160-59ffdb906ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 2\n",
    "voc_size = len(vocabs)\n",
    "model = SkipgramNeg(voc_size, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9c122bc0-3e82-4686-9c8e-99acadaaa9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_tensor = negative_sampling(label_tensor, unigram_table, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5a391509-aedd-45e5-b836-43d8a74e2579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 1]), torch.Size([5, 1]))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor.shape, label_tensor.shape#, neg_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a7ce2cbd-7c86-4efb-b66d-a27c7e1a4bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(input_tensor, label_tensor, neg_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "938e1d9c-e54e-4899-9df6-2383271083fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0161, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9717f2ed-6dad-4b8d-937f-4f1fa9dcba07",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "345e37e0-3860-4852-b836-5da72b62645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the model will accept three vectors - u_c, u_j, v_mean\n",
    "#u_c - vector for center word from the output word matrix\n",
    "#u_j - vectors for all vocab from the output word matrix\n",
    "#v_mean - mean of the vectors of context words from the input word matrix \n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    \n",
    "    def __init__(self, voc_size, emb_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.input_word  = nn.Embedding(voc_size, emb_size) #v\n",
    "        self.output_word = nn.Embedding(voc_size, emb_size) #u\n",
    "    \n",
    "    def forward(self, center_word, outside_word, all_vocabs):\n",
    "        #center_word: (batch_size, 1)\n",
    "        #context_words: (batch_size, window_size * 2)\n",
    "        #all_vocabs: (batch_size, voc_size)\n",
    "        batch_size = center_word.shape[0]\n",
    "        \n",
    "        #convert them into embedding\n",
    "        center_word_embed  = self.output_word(center_word)   #(batch_size, 1, emb_size)\n",
    "        outside_word_embed = self.input_word(outside_word)   #(batch_size, window_size * 2, emb_size)\n",
    "        all_vocabs_embed   = self.output_word(all_vocabs)    #(batch_size, voc_size, emb_size)        \n",
    "        \n",
    "        # mean of input word embeddings\n",
    "        v_mean = torch.sum(outside_word_embed, 1) / len(outside_word) #(batch_size, emb_size)\n",
    "        \n",
    "        ucv = center_word_embed.bmm(v_mean.reshape(batch_size, 1, self.emb_size).transpose(1, 2)).squeeze()\n",
    "        #(batch_size, 1, emb_size) @ (batch_size, emb_size, 1) = (batch_size, 1, 1) ==> (batch_size, 1)\n",
    "        \n",
    "        ujv = all_vocabs_embed.bmm(v_mean.reshape(batch_size, 1, self.emb_size).transpose(1, 2)).squeeze(2)\n",
    "        #(batch_size, voc_size, emb_size) @ (batch_size, emb_size, 1) = (batch_size, voc_size)\n",
    "        \n",
    "        ujv_log_exp = torch.log(torch.exp(ujv))\n",
    "        # (batch_size, voc_size) -> (batch_size,)\n",
    "        \n",
    "        loss_fn = - ucv + torch.sum(ujv_log_exp, 1)\n",
    "        # - (batch_size, 1) + (batch_size, 1) = (batch_size, 1)\n",
    "        \n",
    "        return torch.mean(loss_fn) # scaler for back-propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61409d2a-9b4d-4082-a9b2-8c08103ce878",
   "metadata": {},
   "source": [
    "<b>Test Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4e98af1b-4716-4e57-99c3-5a816cafbdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(5, voc_size)\n",
    "model = CBOW(voc_size, emb_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "960aa485-b463-4dd6-ae02-33191794f562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([5, 1]), torch.Size([5, 4]))"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input, label = random_batch_cbow(5, corpus_tokenized, skipgrams_CBOW)\n",
    "input_tensor = torch.LongTensor(input)  \n",
    "label_tensor = torch.LongTensor(label)\n",
    "input_tensor.shape, label_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4f329d07-eaf9-4095-b68b-97f0fb79c671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1695]), tensor([6908, 2849, 7007, 4609]))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor[0], label_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "fc1da918-54c5-434c-89e3-6ad813a19815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.0261, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(input_tensor, label_tensor, all_vocabs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3040d20-71e5-4335-8fda-cd6f4ba94d18",
   "metadata": {},
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f97bfea2-65e7-4e2b-b0de-0810a6725a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GloVe(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size,embed_size):\n",
    "        super(GloVe,self).__init__()\n",
    "        self.embedding_v = nn.Embedding(vocab_size, embed_size) # center embedding\n",
    "        self.embedding_u = nn.Embedding(vocab_size, embed_size) # out embedding\n",
    "        \n",
    "        self.v_bias = nn.Embedding(vocab_size, 1)\n",
    "        self.u_bias = nn.Embedding(vocab_size, 1)\n",
    "        \n",
    "    def forward(self, center_words, target_words, coocs, weighting):\n",
    "        center_embeds = self.embedding_v(center_words) # [batch_size, 1, emb_size]\n",
    "        target_embeds = self.embedding_u(target_words) # [batch_size, 1, emb_size]\n",
    "        \n",
    "        center_bias = self.v_bias(center_words).squeeze(1)\n",
    "        target_bias = self.u_bias(target_words).squeeze(1)\n",
    "        \n",
    "        inner_product = target_embeds.bmm(center_embeds.transpose(1, 2)).squeeze(2)\n",
    "        #[batch_size, 1, emb_size] @ [batch_size, emb_size, 1] = [batch_size, 1, 1] = [batch_size, 1]\n",
    "        \n",
    "        #note that coocs already got log\n",
    "        loss = weighting*torch.pow(inner_product +center_bias + target_bias - coocs, 2)\n",
    "        \n",
    "        return torch.sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489070de-1f3e-4a67-8499-b4f42970593d",
   "metadata": {},
   "source": [
    "<b>Test Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "74df0d48-f3d2-4163-af38-539084f1b265",
   "metadata": {},
   "outputs": [],
   "source": [
    "input, target, cooc, weightin = random_batch_gloVe(5, corpus_tokenized, gloVes, X_ik, weighting_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3deea44f-4c3e-498d-a245-aca01073a368",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size = 2\n",
    "voc_size = len(vocabs)\n",
    "model = GloVe(voc_size, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2dbb0dca-cda2-4979-ba7a-0b7a2d74c683",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_batch    = torch.LongTensor(input)\n",
    "target_batch   = torch.LongTensor(target)\n",
    "cooc_batch     = torch.FloatTensor(cooc)\n",
    "weightin_batch = torch.FloatTensor(weightin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "52b4e09e-d818-4dac-9a59-178dcda36d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model(input_batch, target_batch, cooc_batch, weightin_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cc447d6a-12e6-480d-89c1-023c489060ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3809, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb74c658-04f8-4205-807a-cd94d44c9dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea39d504-b001-4e2c-a1bd-239e58d532ed",
   "metadata": {},
   "source": [
    "## 4) Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ab5e51e4-c114-4197-8d52-3388f72cafa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "43518021-d2b2-45e0-99b1-53708492cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeb95c9-a113-4117-bb94-eb064c282958",
   "metadata": {},
   "source": [
    "### Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "73e614ef-fcf7-4ac9-9475-f0672429af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size   = 50\n",
    "model1     = Skipgram(voc_size, emb_size)\n",
    "optimizer  = optim.Adam(model.parameters(), lr=0.001)\n",
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0a7d100c-2b03-4464-83a3-1a5d998c24b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 8030])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "all_vocabs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9aac3c51-a3da-4f01-9ec7-30060f2e647d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250 | Loss: 27.133650 | Time: 0.0624232292175293\n",
      "Epoch 500 | Loss: 25.043547 | Time: 0.05271458625793457\n",
      "Epoch 750 | Loss: 30.269293 | Time: 0.05938458442687988\n",
      "Epoch 1000 | Loss: 25.299688 | Time: 0.10424232482910156\n",
      "Epoch 1250 | Loss: 28.746025 | Time: 0.051509857177734375\n",
      "Epoch 1500 | Loss: 28.257807 | Time: 0.0509343147277832\n",
      "Epoch 1750 | Loss: 25.788721 | Time: 0.05037236213684082\n",
      "Epoch 2000 | Loss: 26.517616 | Time: 0.05024242401123047\n",
      "Epoch 2250 | Loss: 28.925938 | Time: 0.07238578796386719\n",
      "Epoch 2500 | Loss: 28.873737 | Time: 0.05432248115539551\n",
      "Epoch 2750 | Loss: 24.266396 | Time: 0.0474543571472168\n",
      "Epoch 3000 | Loss: 30.304535 | Time: 0.054558753967285156\n",
      "Epoch 3250 | Loss: 29.937788 | Time: 0.053843021392822266\n",
      "Epoch 3500 | Loss: 25.381868 | Time: 0.05387735366821289\n",
      "Epoch 3750 | Loss: 27.632574 | Time: 0.05071067810058594\n",
      "Epoch 4000 | Loss: 25.153982 | Time: 0.05628061294555664\n",
      "Epoch 4250 | Loss: 28.964128 | Time: 0.050932884216308594\n",
      "Epoch 4500 | Loss: 24.550501 | Time: 0.05925297737121582\n",
      "Epoch 4750 | Loss: 28.989374 | Time: 0.05312323570251465\n",
      "Epoch 5000 | Loss: 27.600216 | Time: 0.052098989486694336\n",
      "Total trainig time: 284.5691695213318\n"
     ]
    }
   ],
   "source": [
    "train_start_time = time.time()\n",
    "\n",
    "#for epoch\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    #get random batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus_tokenized, skipgrams)\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "    \n",
    "    # print(input_batch.shape, label_batch.shape, all_vocabs.shape)\n",
    "    \n",
    "    #loss = model\n",
    "    loss = model1(input_batch, label_batch, all_vocabs)\n",
    "    \n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    #print epoch loss\n",
    "    if (epoch + 1) % 250 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.6f} | Time: {total_time}\")\n",
    "        \n",
    "total_training_time = time.time() - train_start_time\n",
    "print(\"Total trainig time:\", total_training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cbe0b8-7b13-47e5-b489-bf9b6fc46ece",
   "metadata": {},
   "source": [
    "### Skip-gram with Negative Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ed4e0993-60ac-4b6b-92d2-46020f888cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size  = 50\n",
    "model2 = SkipgramNeg(voc_size, emb_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f76f0f70-641b-409d-b374-42a5154566bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250 | Loss: 8.840124 | Time: 0.011084318161010742\n",
      "Epoch 500 | Loss: 12.489389 | Time: 0.011999368667602539\n",
      "Epoch 750 | Loss: 14.173753 | Time: 0.009119749069213867\n",
      "Epoch 1000 | Loss: 9.965052 | Time: 0.014000892639160156\n",
      "Epoch 1250 | Loss: 11.300850 | Time: 0.016988754272460938\n",
      "Epoch 1500 | Loss: 4.757360 | Time: 0.0112457275390625\n",
      "Epoch 1750 | Loss: 4.753320 | Time: 0.011295318603515625\n",
      "Epoch 2000 | Loss: 7.002831 | Time: 0.01408839225769043\n",
      "Epoch 2250 | Loss: 11.369360 | Time: 0.013231754302978516\n",
      "Epoch 2500 | Loss: 9.133273 | Time: 0.010119199752807617\n",
      "Epoch 2750 | Loss: 10.411145 | Time: 0.013116598129272461\n",
      "Epoch 3000 | Loss: 12.896254 | Time: 0.013008832931518555\n",
      "Epoch 3250 | Loss: 8.752337 | Time: 0.011004924774169922\n",
      "Epoch 3500 | Loss: 10.880651 | Time: 0.011614084243774414\n",
      "Epoch 3750 | Loss: 9.299257 | Time: 0.014136075973510742\n",
      "Epoch 4000 | Loss: 9.748012 | Time: 0.009064912796020508\n",
      "Epoch 4250 | Loss: 5.862792 | Time: 0.010156631469726562\n",
      "Epoch 4500 | Loss: 6.698127 | Time: 0.011124610900878906\n",
      "Epoch 4750 | Loss: 11.031863 | Time: 0.014092445373535156\n",
      "Epoch 5000 | Loss: 9.406975 | Time: 0.01323699951171875\n",
      "Total trainig time: 61.086962938308716\n"
     ]
    }
   ],
   "source": [
    "train_start_time = time.time()\n",
    "\n",
    "#for epoch\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #get random batch\n",
    "    input_batch, label_batch = random_batch(batch_size, corpus_tokenized, skipgrams)\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "    neg_batch   = negative_sampling(label_batch, unigram_table, 5)    \n",
    "    \n",
    "    #loss = model\n",
    "    loss = model2(input_batch, label_batch, neg_batch)\n",
    "    \n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    #print epoch loss\n",
    "    if (epoch + 1) % 250 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.6f} | Time: {total_time}\")\n",
    "        \n",
    "total_training_time = time.time() - train_start_time\n",
    "print(\"Total trainig time:\", total_training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135f400d-e50f-40e3-a424-65d504ebe14a",
   "metadata": {},
   "source": [
    "### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b4730bdd-ce76-404c-877d-54b84f885bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size  = 50\n",
    "model3 = CBOW(voc_size, emb_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "98ee837d-04cf-4aa7-ac0d-5d3de219e1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25, 8030])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_vocabs = prepare_sequence(list(vocabs), word2index).expand(batch_size, voc_size)\n",
    "all_vocabs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ca45c69d-7e6f-4e52-8e50-2c19ddccf6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250 | Loss: 0.235588 | Time: 0.04146552085876465\n",
      "Epoch 500 | Loss: 2.079187 | Time: 0.04926276206970215\n",
      "Epoch 750 | Loss: 2.090953 | Time: 0.04590153694152832\n",
      "Epoch 1000 | Loss: 4.416236 | Time: 0.04063844680786133\n",
      "Epoch 1250 | Loss: 3.061728 | Time: 0.046878814697265625\n",
      "Epoch 1500 | Loss: 7.287611 | Time: 0.048037052154541016\n",
      "Epoch 1750 | Loss: 3.786499 | Time: 0.03947854042053223\n",
      "Epoch 2000 | Loss: 11.088496 | Time: 0.04648327827453613\n",
      "Epoch 2250 | Loss: -2.276132 | Time: 0.046859025955200195\n",
      "Epoch 2500 | Loss: -6.187428 | Time: 0.043962717056274414\n",
      "Epoch 2750 | Loss: 6.884033 | Time: 0.045606136322021484\n",
      "Epoch 3000 | Loss: -14.870471 | Time: 0.043202877044677734\n",
      "Epoch 3250 | Loss: -11.567373 | Time: 0.04187822341918945\n",
      "Epoch 3500 | Loss: 10.264377 | Time: 0.04459786415100098\n",
      "Epoch 3750 | Loss: -4.122556 | Time: 0.048424482345581055\n",
      "Epoch 4000 | Loss: 7.903654 | Time: 0.03985166549682617\n",
      "Epoch 4250 | Loss: 13.996526 | Time: 0.04264545440673828\n",
      "Epoch 4500 | Loss: 8.725114 | Time: 0.05319380760192871\n",
      "Epoch 4750 | Loss: 2.186495 | Time: 0.04383277893066406\n",
      "Epoch 5000 | Loss: 2.631488 | Time: 0.04496312141418457\n",
      "Total trainig time: 227.3456060886383\n"
     ]
    }
   ],
   "source": [
    "train_start_time = time.time()\n",
    "\n",
    "#for epoch\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    #get random batch\n",
    "    input_batch, label_batch = random_batch_cbow(batch_size, corpus_tokenized, skipgrams_CBOW)\n",
    "    input_batch = torch.LongTensor(input_batch)\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "    \n",
    "    #loss = model\n",
    "    loss = model3(input_batch, label_batch, all_vocabs)\n",
    "    \n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    #print epoch loss\n",
    "    if (epoch + 1) % 250 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.6f} | Time: {total_time}\")\n",
    "        \n",
    "total_training_time = time.time() - train_start_time\n",
    "print(\"Total trainig time:\", total_training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0bf010-a1f1-41bc-8f96-def69699abf2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "03356942-11dc-4afa-af94-752f9bd37f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_size  = 50\n",
    "model4 = GloVe(voc_size, emb_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "batch_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b63c1a38-1a62-4159-8fea-d6a0162fe2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 250 | Loss: 2264.771240 | Time: 0.02010178565979004\n",
      "Epoch 500 | Loss: 2286.709961 | Time: 0.020184993743896484\n",
      "Epoch 750 | Loss: 1489.277466 | Time: 0.02032613754272461\n",
      "Epoch 1000 | Loss: 3218.607666 | Time: 0.019252777099609375\n",
      "Epoch 1250 | Loss: 1753.649780 | Time: 0.017228126525878906\n",
      "Epoch 1500 | Loss: 1367.238281 | Time: 0.016101360321044922\n",
      "Epoch 1750 | Loss: 1028.348022 | Time: 0.017450571060180664\n",
      "Epoch 2000 | Loss: 1810.755615 | Time: 0.014273881912231445\n",
      "Epoch 2250 | Loss: 1304.259888 | Time: 0.017575979232788086\n",
      "Epoch 2500 | Loss: 2069.218994 | Time: 0.01716160774230957\n",
      "Epoch 2750 | Loss: 1608.393921 | Time: 0.016108274459838867\n",
      "Epoch 3000 | Loss: 1967.837524 | Time: 0.017348766326904297\n",
      "Epoch 3250 | Loss: 2499.779785 | Time: 0.01599574089050293\n",
      "Epoch 3500 | Loss: 1666.710938 | Time: 0.0143280029296875\n",
      "Epoch 3750 | Loss: 1845.511963 | Time: 0.014918327331542969\n",
      "Epoch 4000 | Loss: 2660.739746 | Time: 0.017229795455932617\n",
      "Epoch 4250 | Loss: 1971.471680 | Time: 0.014204025268554688\n",
      "Epoch 4500 | Loss: 3685.251465 | Time: 0.017220258712768555\n",
      "Epoch 4750 | Loss: 1360.844604 | Time: 0.01415395736694336\n",
      "Epoch 5000 | Loss: 2601.044434 | Time: 0.014997482299804688\n",
      "Total trainig time: 86.27384781837463\n"
     ]
    }
   ],
   "source": [
    "train_start_time = time.time()\n",
    "\n",
    "#for epoch\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    #get random batch\n",
    "    input, target, cooc, weightin = random_batch_gloVe(batch_size, corpus_tokenized, gloVes, X_ik, weighting_dic)\n",
    "    input_batch    = torch.LongTensor(input)\n",
    "    target_batch   = torch.LongTensor(target)\n",
    "    cooc_batch     = torch.FloatTensor(cooc)\n",
    "    weightin_batch = torch.FloatTensor(weightin)\n",
    "        \n",
    "    #loss = model\n",
    "    loss = model4(input_batch, target_batch, cooc_batch, weightin_batch)\n",
    "    \n",
    "    #backpropagate\n",
    "    loss.backward()\n",
    "    \n",
    "    #update alpha\n",
    "    optimizer.step()\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    #print epoch loss\n",
    "    if (epoch + 1) % 250 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.6f} | Time: {total_time}\")\n",
    "        \n",
    "total_training_time = time.time() - train_start_time\n",
    "print(\"Total trainig time:\", total_training_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29a1fef-ebb0-4859-9a0a-27da71019313",
   "metadata": {},
   "source": [
    "## 5) Evaluating Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f06c8a-091e-456c-8ecc-b132599bd0ea",
   "metadata": {},
   "source": [
    "Since an analogy task dataset is being used, it contains questions in form of  \"a is to b as c is to ?\". By identifying the fourth word, accuracy for both semantic and syntactic parts will check. Suppose A, B, and C are the embeddings of words a, b, and c. The word whose embedding gives the highest cosine similarity (B - A + C) would be the missing term to the question. Therefore, every word in the vocabulary is tried. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35517a53-c536-4697-b202-4eb7fdcee34d",
   "metadata": {},
   "source": [
    "After doing an evalution, unfortunately it was found that none of the models identified the missing term correctly for both semantic and syntactic parts. This may be due to limited corpus size and vocabulary. Due to this, another accuracy based on how the correct missing term is ranked based on similarity is used. The average was taken from very analogy task questions for both semantic and syntactic parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "067cd02d-8cca-46ea-87ca-b14b49527370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed(word, model):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        index = word2index['<UNK>']\n",
    "    \n",
    "    word = torch.LongTensor([index])\n",
    "\n",
    "    center_embed  = model.embedding_center_word(word)\n",
    "    outside_embed = model.embedding_outside_word(word)\n",
    "    \n",
    "    embed = (center_embed + outside_embed) / 2\n",
    "    \n",
    "    return embed\n",
    "    # return  embed[0][0].item(), embed[0][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8c8e3c9d-d8af-4d4e-a28a-c4c871235883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed(\"man\", model1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4fd4d7ac-6031-4de1-a5e6-13fb309be4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_CBOW(word, model):\n",
    "    try:\n",
    "        index = word2index[word]\n",
    "    except:\n",
    "        index = word2index['<UNK>']\n",
    "    \n",
    "    word = torch.LongTensor([index])\n",
    "\n",
    "    center_embed  = model.input_word(word)\n",
    "    outside_embed = model.output_word(word)\n",
    "    \n",
    "    embed = (center_embed + outside_embed) / 2\n",
    "    \n",
    "    return embed\n",
    "    # return  embed[0][0].item(), embed[0][1].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9612dec8-4172-4b7f-8930-8bb72d26637b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed_CBOW(\"man\", model3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "92d8e606-5863-44a1-af75-467470a2530f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embed_GloVe(word, model):\n",
    "    id_tensor = torch.LongTensor([word2index[word]])\n",
    "    v_embed = model.embedding_v(id_tensor)\n",
    "    u_embed = model.embedding_u(id_tensor) \n",
    "    word_embed = (v_embed + u_embed) / 2 \n",
    "    \n",
    "    return word_embed\n",
    "\n",
    "    # x, y = word_embed[0][0].item(), word_embed[0][1].item()\n",
    "    # return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "6710af96-8b97-4465-8579-084b68bdc007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_embed_GloVe(\"man\", model4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "67cb991b-af10-4634-88c9-a8cc523f325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a, b):\n",
    "    cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01afa07-97c1-4370-8146-3d2b3ceb39cc",
   "metadata": {},
   "source": [
    "### Load and Prepare Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2026d360-9d5b-4391-9076-8ec09f231d2d",
   "metadata": {},
   "source": [
    "https://www.pythontutorial.net/python-basics/python-read-text-file/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5d8e8147-97c6-4fe9-ae63-11453559727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('questions-words.txt') as f:\n",
    "    testDataset = [line.strip() for line in f.readlines() if line[0] != \":\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "46004fd4-0e0d-4902-aaf1-172775122e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataset = {}\n",
    "key         = None\n",
    "value       = []\n",
    "with open('questions-words.txt') as f:\n",
    "    for line in f.readlines():\n",
    "        if line[0] == \":\":\n",
    "            if key != None:\n",
    "                testDataset[key] = value\n",
    "            key   = line.strip()\n",
    "            value = []\n",
    "            continue\n",
    "        value.append(line.strip().lower())\n",
    "    testDataset[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "ce8ced12-d840-45d0-907c-fd1efb1e4410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a2c18e02-9761-443a-a825-91bb0ddb6ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([': capital-common-countries', ': capital-world', ': currency', ': city-in-state', ': family', ': gram1-adjective-to-adverb', ': gram2-opposite', ': gram3-comparative', ': gram4-superlative', ': gram5-present-participle', ': gram6-nationality-adjective', ': gram7-past-tense', ': gram8-plural', ': gram9-plural-verbs'])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b26a7721-8ce1-448f-99de-84ccf20004b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['athens greece baghdad iraq',\n",
       " 'athens greece bangkok thailand',\n",
       " 'athens greece beijing china',\n",
       " 'athens greece berlin germany',\n",
       " 'athens greece bern switzerland',\n",
       " 'athens greece cairo egypt',\n",
       " 'athens greece canberra australia',\n",
       " 'athens greece hanoi vietnam',\n",
       " 'athens greece havana cuba',\n",
       " 'athens greece helsinki finland']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDataset[': capital-common-countries'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "a5008d86-2c9b-47e9-a050-369668880897",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic  = [': capital-common-countries', ': capital-world', ': currency', ': family']\n",
    "syntactic = [': gram1-adjective-to-adverb', ': gram2-opposite', ': gram3-comparative', ': gram4-superlative', \n",
    "             ': gram5-present-participle', ': gram6-nationality-adjective', ': gram7-past-tense', ': gram8-plural', \n",
    "             ': gram9-plural-verbs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2444c253-fb4d-44b8-8580-1222958afd65",
   "metadata": {},
   "source": [
    "### Semantic Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "192f14a8-3fbc-463e-84c1-97fdcc840de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_syntatic_eval(model, testDict, CBOWMode = False, GloVeMode = False):\n",
    "    total_corr = 0\n",
    "    pairs_used = 0\n",
    "    acc_sum    = 0\n",
    "    \n",
    "    for key in testDict:\n",
    "        pairs = testDataset[key]\n",
    "        # print(pairs[0].split(\" \"))\n",
    "        for pair in pairs:\n",
    "            pair_tokenized = pair.split(\" \")\n",
    "            word_a, word_b, word_c, word_d = pair_tokenized  \n",
    "            if word_a not in word2index or word_b not in word2index or word_c not in word2index or word_d not in word2index:\n",
    "                continue\n",
    "                \n",
    "            word_d_index = vocabs.index(word_d)\n",
    "            \n",
    "            if GloVeMode:\n",
    "                a_embedding = get_embed_GloVe(word_a, model) \n",
    "                b_embedding = get_embed_GloVe(word_b, model)\n",
    "                c_embedding = get_embed_GloVe(word_c, model)\n",
    "            elif CBOWMode:\n",
    "                a_embedding = get_embed_CBOW(word_a, model) \n",
    "                b_embedding = get_embed_CBOW(word_b, model)\n",
    "                c_embedding = get_embed_CBOW(word_c, model)\n",
    "            else:\n",
    "                a_embedding = get_embed(word_a, model) \n",
    "                b_embedding = get_embed(word_b, model)\n",
    "                c_embedding = get_embed(word_c, model)\n",
    "            \n",
    "            AminusBplusC = (b_embedding - a_embedding + c_embedding).squeeze()\n",
    "            \n",
    "            count = -1\n",
    "            similarity_arr = [0] * len(vocabs)\n",
    "            for vocab in vocabs:\n",
    "                count += 1\n",
    "                if vocab in pair_tokenized[:3]:\n",
    "                    continue\n",
    "                \n",
    "                if GloVeMode:\n",
    "                    current = get_embed_GloVe(vocab, model).squeeze()\n",
    "                elif CBOWMode:\n",
    "                    current = get_embed_CBOW(vocab, model).squeeze()\n",
    "                else:\n",
    "                    current = get_embed(vocab, model).squeeze()\n",
    "                similarity_arr[count] = cos_sim(AminusBplusC.detach().numpy(), current.detach().numpy())\n",
    "            \n",
    "            similarity_arr_sorted_index = np.argsort(similarity_arr)\n",
    "            rank                        = np.where(similarity_arr_sorted_index == word_d_index)[0][0]\n",
    "            acc_sum += (rank + 1) / len(vocabs)\n",
    "            \n",
    "            pairs_used += 1\n",
    "            predicted_word = np.argmax(similarity_arr)\n",
    "            if predicted_word == word_d:\n",
    "                total_corr+= 1\n",
    "                \n",
    "    total_acc = total_corr / pairs_used\n",
    "    avg_acc = acc_sum / pairs_used\n",
    "    return total_acc, total_corr, avg_acc, pairs_used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf32ded-3562-473f-8a30-37f2f7ea0683",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "993f394e-b1d2-4976-8cf1-afb5f71f6172",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc, total_corr, avg_acc, pairs_used = semantic_syntatic_eval(model1, semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "db8d757a-a822-4965-ae5b-4ce391c5d9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.0%\n",
      "Average accuracy according to the ranking: 0.49%\n",
      "Total pairs used: 165\n"
     ]
    }
   ],
   "source": [
    "print(f\"Overall Accuracy: {total_acc}%\")\n",
    "print(f\"Average accuracy according to the ranking: {round(avg_acc, 2)}%\")\n",
    "print(f\"Total pairs used:\", pairs_used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b23d09e-1de5-4a01-8a39-c625a38f9d02",
   "metadata": {},
   "source": [
    "##### Skip-gram with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6b489845-c132-49a6-aafb-1c17fe88990d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc_neg, total_corr_neg, avg_acc_neg, pairs_used_neg = semantic_syntatic_eval(model2, semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "4ed29cc9-fd53-485a-bd2c-e93082f5bab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.0%\n",
      "Average accuracy according to the ranking: 0.57%\n",
      "Total pairs used: 165\n"
     ]
    }
   ],
   "source": [
    "print(f\"Overall Accuracy: {total_acc_neg}%\")\n",
    "print(f\"Average accuracy according to the ranking: {round(avg_acc_neg, 2)}%\")\n",
    "print(f\"Total pairs used:\", pairs_used_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8347861-e67c-4283-8237-ff0e37ab2d8c",
   "metadata": {},
   "source": [
    "##### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "60fd04b9-f963-4e50-8733-d0c3ecc8b04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc_cbow, total_corr_cbow, avg_acc_cbow, pairs_used_cbow = semantic_syntatic_eval(model3, semantic, CBOWMode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "e3e25492-cd46-41a4-94ae-14d56fe3ddc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.0%\n",
      "Average accuracy according to the ranking: 0.45%\n",
      "Total pairs used: 165\n"
     ]
    }
   ],
   "source": [
    "print(f\"Overall Accuracy: {total_acc_cbow}%\")\n",
    "print(f\"Average accuracy according to the ranking: {round(avg_acc_cbow, 2)}%\")\n",
    "print(f\"Total pairs used:\", pairs_used_cbow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d329db-f128-4eea-b36f-e477a7f1162d",
   "metadata": {},
   "source": [
    "##### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "341f2f9e-3227-4908-8483-b1d197ebf89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc_gloVe, total_corr_gloVe, avg_acc_gloVe, pairs_used_gloVe = semantic_syntatic_eval(model4, semantic, GloVeMode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4a864e4c-94b8-4568-852d-45cfd04e490c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.0%\n",
      "Average accuracy according to the ranking: 0.4%\n",
      "Total pairs used: 165\n"
     ]
    }
   ],
   "source": [
    "print(f\"Overall Accuracy: {total_acc_gloVe}%\")\n",
    "print(f\"Average accuracy according to the ranking: {round(avg_acc_gloVe, 2)}%\")\n",
    "print(f\"Total pairs used:\", pairs_used_gloVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55cbca5-34e1-4433-a165-1d8d01ed675a",
   "metadata": {},
   "source": [
    "### Syntatic Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c5b1ea-be6b-45e6-9074-4cfdebf3d1cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "95b95b0e-eb83-409f-a735-45d75d209424",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc2, total_corr2, avg_acc2, pairs_used2 = semantic_syntatic_eval(model1, syntactic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d452d949-ecf4-4c40-a97c-3d3b0bb8a663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.0%\n",
      "Average accuracy according to the ranking: 0.48%\n",
      "Total pairs used: 1266\n"
     ]
    }
   ],
   "source": [
    "print(f\"Overall Accuracy: {total_acc2}%\")\n",
    "print(f\"Average accuracy according to the ranking: {round(avg_acc2, 2)}%\")\n",
    "print(f\"Total pairs used:\", pairs_used2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab445d-4f36-4f3e-aafc-6ef9d045d9d2",
   "metadata": {},
   "source": [
    "##### Skip-gram with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b7daca42-f575-44ff-b539-348e6fc51010",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc_neg2, total_corr_neg2, avg_acc_neg2, pairs_used_neg2 = semantic_syntatic_eval(model2, syntactic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "908b7d37-4b34-4e08-ba22-296618df3d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.0%\n",
      "Average accuracy according to the ranking: 0.5%\n",
      "Total pairs used: 1266\n"
     ]
    }
   ],
   "source": [
    "print(f\"Overall Accuracy: {total_acc_neg2}%\")\n",
    "print(f\"Average accuracy according to the ranking: {round(avg_acc_neg2, 2)}%\")\n",
    "print(f\"Total pairs used:\", pairs_used_neg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5122e9-08bc-40a8-8ecf-6756033b7ceb",
   "metadata": {},
   "source": [
    "##### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "03e62aec-5a46-48a9-9051-216ad938caa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc_cbow2, total_corr_cbow2, avg_acc_cbow2, pairs_used_cbow2 = semantic_syntatic_eval(model3, syntactic, CBOWMode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b21c2b-c359-40aa-a917-3a9964027247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.0%\n",
      "Average accuracy according to the ranking: 0.51%\n",
      "Total pairs used: 1266\n"
     ]
    }
   ],
   "source": [
    "print(f\"Overall Accuracy: {total_acc_cbow2}%\")\n",
    "print(f\"Average accuracy according to the ranking: {round(avg_acc_cbow2, 2)}%\")\n",
    "print(f\"Total pairs used:\", pairs_used_cbow2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d94a0f-26bb-436b-b0ce-453602f21148",
   "metadata": {},
   "source": [
    "##### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aadc2a-c99d-4a14-8aa4-1e770f330646",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc_gloVe2, total_corr_gloVe2, avg_acc_gloVe2, pairs_used_gloVe2 = semantic_syntatic_eval(model4, syntactic, GloVeMode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b965c762-2054-4ede-a552-23648fe8ee56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.0%\n",
      "Average accuracy according to the ranking: 0.48%\n",
      "Total pairs used: 1266\n"
     ]
    }
   ],
   "source": [
    "print(f\"Overall Accuracy: {total_acc_gloVe2}%\")\n",
    "print(f\"Average accuracy according to the ranking: {round(avg_acc_gloVe2, 2)}%\")\n",
    "print(f\"Total pairs used:\", pairs_used_gloVe2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c20c12-81c4-46ec-9acd-603653f7a3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be322b11-e0d8-476e-9777-195a573fa5ff",
   "metadata": {},
   "source": [
    "### ----- Semantic and Syntatic Results Summary -----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985d66aa-a11c-464c-a507-22b5d6b8a367",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/how-to-make-a-table-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fb22c4-bc7a-4585-9485-d26dc3b21684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9890a243-c035-4a05-b909-1acac852b735",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pairs_semantic = 0\n",
    "for key in semantic:\n",
    "    total_pairs_semantic += len(testDataset[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f3df4b-3d32-4f52-9a2f-f60f573e68ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs for semantic evaluation: 6402\n",
      "Pairs used for semantic evaluation: 165\n"
     ]
    }
   ],
   "source": [
    "print(\"Total pairs for semantic evaluation:\", total_pairs_semantic)\n",
    "print(\"Pairs used for semantic evaluation:\", pairs_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6a0b67-4540-4f54-9f2e-667bfbca7f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pairs_syntactic = 0\n",
    "for key in syntactic:\n",
    "    total_pairs_syntactic += len(testDataset[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d7789-e7bc-4b7e-878c-ea31dc8c0b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total pairs for syntactic evaluation: 10675\n",
      "Pairs used for syntactic evaluatio: 1266\n"
     ]
    }
   ],
   "source": [
    "print(\"Total pairs for syntactic evaluation:\", total_pairs_syntactic)\n",
    "print(\"Pairs used for syntactic evaluatio:\", pairs_used2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67128da4-7902-4e08-85d2-005a7d6c0651",
   "metadata": {},
   "source": [
    "### Total Accuracy (%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ee2d10-49b6-4580-a3f5-96327b3f0e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "head    = [\"Model\", \"Semantic\", \"Syntatic\"]\n",
    "content = [\n",
    "    [\"Skip-gram\", total_acc, total_acc2],\n",
    "    [\"Skip-gram with negative sampling\", total_acc_neg, total_acc_neg2],\n",
    "    [\"CBOW\", total_acc_cbow, total_acc_cbow2],\n",
    "    [\"GloVe\", total_acc_gloVe, total_acc_gloVe2]\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a150735f-f727-48c5-8f95-0d2a28467934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+------------+------------+\n",
      "| Model                            |   Semantic |   Syntatic |\n",
      "+==================================+============+============+\n",
      "| Skip-gram                        |          0 |          0 |\n",
      "+----------------------------------+------------+------------+\n",
      "| Skip-gram with negative sampling |          0 |          0 |\n",
      "+----------------------------------+------------+------------+\n",
      "| CBOW                             |          0 |          0 |\n",
      "+----------------------------------+------------+------------+\n",
      "| GloVe                            |          0 |          0 |\n",
      "+----------------------------------+------------+------------+\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(content, headers=head, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e8794b-190e-470d-84ec-34c714efc7e2",
   "metadata": {},
   "source": [
    "### Average Accuracy (%) based on Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fbb70f-4452-4fde-bac3-48a9c3410a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "head    = [\"Model\", \"Semantic\", \"Syntatic\"]\n",
    "content = [\n",
    "    [\"Skip-gram\", avg_acc, avg_acc2],\n",
    "    [\"Skip-gram with negative sampling\", avg_acc_neg, avg_acc_neg2],\n",
    "    [\"CBOW\", avg_acc_cbow, avg_acc_cbow2],\n",
    "    [\"GloVe\", avg_acc_gloVe, avg_acc_gloVe2]\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784b8147-aba2-4ace-9f9a-0534ee39e9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+------------+------------+\n",
      "| Model                            |   Semantic |   Syntatic |\n",
      "+==================================+============+============+\n",
      "| Skip-gram                        |   0.510768 |   0.487203 |\n",
      "+----------------------------------+------------+------------+\n",
      "| Skip-gram with negative sampling |   0.55887  |   0.48972  |\n",
      "+----------------------------------+------------+------------+\n",
      "| CBOW                             |   0.581599 |   0.514867 |\n",
      "+----------------------------------+------------+------------+\n",
      "| GloVe                            |   0.532616 |   0.479572 |\n",
      "+----------------------------------+------------+------------+\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(content, headers=head, tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c282882b-34df-43de-8ab9-3f25bce09469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "309255e6-7584-4ffd-ad55-d6d7ed04bbdd",
   "metadata": {},
   "source": [
    "### --- Similarity Test on Word Pairs from WordSim353 Dataset ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28246af-3703-46f9-bb85-ab3d3bc9dcd5",
   "metadata": {},
   "source": [
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b725adb1-6091-45dd-8904-e47a21e0d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2532faa8-c56c-4a94-9158-be37fe0c788b",
   "metadata": {},
   "source": [
    "### Load Similarity Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9214c63-cd2f-4997-95bf-ff7504b8691a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wordsim353_sim_rel/wordsim_relatedness_goldstandard.txt') as f:\n",
    "    relatedness_db = [line.strip().lower().split(\"\\t\") for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ef9992-cd84-41ed-b311-ff704c5aa7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['computer', 'keyboard', '7.62'],\n",
       " ['jerusalem', 'israel', '8.46'],\n",
       " ['planet', 'galaxy', '8.11'],\n",
       " ['canyon', 'landscape', '7.53'],\n",
       " ['opec', 'country', '5.63'],\n",
       " ['day', 'summer', '3.94'],\n",
       " ['day', 'dawn', '7.53'],\n",
       " ['country', 'citizen', '7.31'],\n",
       " ['planet', 'people', '5.75'],\n",
       " ['environment', 'ecology', '8.81']]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relatedness_db[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5688bc2-122d-4b86-bd27-3f4577c3d48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wordsim353_sim_rel/wordsim_similarity_goldstandard.txt') as f:\n",
    "    similarity_db = [line.strip().lower().split(\"\\t\") for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963d7b66-cde7-4d02-a75e-81f83ec54a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tiger', 'cat', '7.35'],\n",
       " ['tiger', 'tiger', '10.00'],\n",
       " ['plane', 'car', '5.77'],\n",
       " ['train', 'car', '6.31'],\n",
       " ['television', 'radio', '6.77'],\n",
       " ['media', 'radio', '7.42'],\n",
       " ['bread', 'butter', '6.19'],\n",
       " ['cucumber', 'potato', '5.92'],\n",
       " ['doctor', 'nurse', '7.00'],\n",
       " ['professor', 'doctor', '6.62']]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_db[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2fd73d-ba05-4599-9f2b-e3271b0f8b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_eval(dataset, model, CBOWMode = False, GloVeMode = False):\n",
    "    valid_idx     = []\n",
    "    cos_sim_arr   = []\n",
    "    human_val_sim = []\n",
    "    \n",
    "    for idx, pair in enumerate(dataset):\n",
    "        word1, word2, evalHuman = pair\n",
    "        if word1 not in vocabs or word2 not in vocabs:\n",
    "            continue\n",
    "        \n",
    "        if GloVeMode:\n",
    "            word1_embedding = get_embed_GloVe(word1, model)\n",
    "            word2_embedding = get_embed_GloVe(word2, model)\n",
    "        elif CBOWMode:\n",
    "            word1_embedding = get_embed_CBOW(word1, model)\n",
    "            word2_embedding = get_embed_CBOW(word2, model)\n",
    "        else:\n",
    "            word1_embedding = get_embed(word1, model)\n",
    "            word2_embedding = get_embed(word2, model)\n",
    "        \n",
    "        sim_eval = cos_sim(word1_embedding.detach().numpy().squeeze(), word2_embedding.detach().numpy().squeeze())\n",
    "        valid_idx.append(idx)\n",
    "        cos_sim_arr.append(sim_eval)\n",
    "        human_val_sim.append(float(evalHuman))\n",
    "        \n",
    "        res = stats.spearmanr(cos_sim_arr, human_val_sim)\n",
    "    \n",
    "    return valid_idx, cos_sim_arr, human_val_sim, res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e010470-1084-45a3-a749-453e9a490275",
   "metadata": {},
   "source": [
    "### Spearman Correlation Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf467a60-f30b-4270-832b-2df96e70579e",
   "metadata": {},
   "source": [
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8784c4c-5155-419d-adec-7e7be54a2807",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=JwNwbu-g2m0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc95c4-8208-4307-ab3d-a66ad028d9a8",
   "metadata": {},
   "source": [
    "Spearman correlation test is the measurement of the \"monotonicity of the relationship between two datasets\". From this test, two paramters, Spearman correlation coefficient (rs) and p-value are returned. Spearman correlation coefficient (rs) gives information on how strong two dataset are un-linearlu correlated with range -1 to 1. On the order hand,  p-value suggests how strongly two datasets are not correlated with range 0 to 1. Since null hypothesis is used, values closer to 1 would strongly indicate that the two datasets are uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7977c-3ad7-4a8f-8356-989bea1a22e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5912aa5d-e607-4b18-bcf8-1947cc3234f7",
   "metadata": {},
   "source": [
    "##### Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb2806d-161b-48d5-aeec-3213d1a4d2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idx, cos_sim_arr, human_val_sim, res = similarity_eval(similarity_db, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf7d949-ef37-4f4a-8d1a-fade29b9aaeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 203\n",
      "Samples used: 88\n",
      "Spearmanr correlation coefficient (rs): 0.1381\n",
      "p-value: 0.1995\n"
     ]
    }
   ],
   "source": [
    "print(\"Total samples:\", len(similarity_db))\n",
    "print(\"Samples used:\", len(valid_idx))\n",
    "print(\"Spearmanr correlation coefficient (rs):\", round(res.correlation,4))\n",
    "print(\"p-value:\", round(res.pvalue,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625bac3b-9d16-4bb3-a1ed-12675c1459e4",
   "metadata": {},
   "source": [
    "##### Skip-gram with negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a666104-9348-49ac-bbcb-51ceb46a314e",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idx_neg, cos_sim_arr_neg, human_val_sim_neg, res_neg = similarity_eval(similarity_db, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f160d21-f558-4f84-9cf4-fee8b31323ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 203\n",
      "Samples used: 88\n",
      "Spearmanr correlation coefficient (rs): 0.1189\n",
      "p-value: 0.27\n"
     ]
    }
   ],
   "source": [
    "print(\"Total samples:\", len(similarity_db))\n",
    "print(\"Samples used:\", len(valid_idx_neg))\n",
    "print(\"Spearmanr correlation coefficient (rs):\", round(res_neg.correlation,4))\n",
    "print(\"p-value:\", round(res_neg.pvalue,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19c8a2f-dba5-493c-b58c-61451298a5a7",
   "metadata": {},
   "source": [
    "##### CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b05bf3-1a0c-4857-8b94-b4590ad2d0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idx_CBOW, cos_sim_arr_CBOW, human_val_sim_CBOW, res_CBOW = similarity_eval(similarity_db, model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a95138b-adbe-4210-a563-50011b9fcf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 203\n",
      "Samples used: 88\n",
      "Spearmanr correlation coefficient (rs):: 0.1189\n",
      "p-value of Spearmanr: 0.27\n"
     ]
    }
   ],
   "source": [
    "print(\"Total samples:\", len(similarity_db))\n",
    "print(\"Samples used:\", len(valid_idx_CBOW))\n",
    "print(\"Spearmanr correlation coefficient (rs)::\", round(res_CBOW.correlation,4))\n",
    "print(\"p-value of Spearmanr:\", round(res_CBOW.pvalue,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061743b6-fd73-4319-8a8e-0dc9bd4b7853",
   "metadata": {},
   "source": [
    "##### GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be512e9-1546-4b4d-a12b-2513213e0441",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_idx_gloVe, cos_sim_arr_gloVe, human_val_sim_gloVe, res_gloVe = similarity_eval(similarity_db, model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d588a8-45ed-4af2-8cfa-328097082ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 203\n",
      "Samples used: 88\n",
      "Spearmanr correlation coefficient (rs):: 0.1381\n",
      "p-value: 0.1995\n"
     ]
    }
   ],
   "source": [
    "print(\"Total samples:\", len(similarity_db))\n",
    "print(\"Samples used:\", len(valid_idx_gloVe))\n",
    "print(\"Spearmanr correlation coefficient (rs)::\", round(res_gloVe.correlation,4))\n",
    "print(\"p-value:\", round(res_gloVe.pvalue,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31060654-cf12-4b21-8cd7-a39594076fd8",
   "metadata": {},
   "source": [
    "### ----- Similarity Test Results -----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d6786d-9440-451c-b0f8-cbb7710fb284",
   "metadata": {},
   "outputs": [],
   "source": [
    "head    = [\"Model\", \"rs\", \"p-value\"]\n",
    "content = [\n",
    "    [\"Skip-gram\", round(res.correlation,4), round(res.pvalue,4)],\n",
    "    [\"Skip-gram with negative sampling\", round(res_neg.correlation,4), round(res_neg.pvalue,4)],\n",
    "    [\"CBOW\", round(res_CBOW.correlation,4), round(res_CBOW.pvalue,4)],\n",
    "    [\"GloVe\", round(res_gloVe.correlation,4), round(res_gloVe.pvalue,4)]\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006f4dc6-0750-4686-ac84-be9d97c91e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------+-----------+\n",
      "| Model                            |     rs |   p-value |\n",
      "+==================================+========+===========+\n",
      "| Skip-gram                        | 0.1381 |    0.1995 |\n",
      "+----------------------------------+--------+-----------+\n",
      "| Skip-gram with negative sampling | 0.1189 |    0.27   |\n",
      "+----------------------------------+--------+-----------+\n",
      "| CBOW                             | 0.1189 |    0.27   |\n",
      "+----------------------------------+--------+-----------+\n",
      "| GloVe                            | 0.1381 |    0.1995 |\n",
      "+----------------------------------+--------+-----------+\n",
      "Where rs is Spearman correlation coefficient\n"
     ]
    }
   ],
   "source": [
    "print(tabulate(content, headers=head, tablefmt=\"grid\"))\n",
    "print(\"Where rs is Spearman correlation coefficient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a1ce1-a4e3-4b25-a644-8f175a60fcb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cbcf190-37e0-4ead-b9ce-2819b801017d",
   "metadata": {},
   "source": [
    "## 6) Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b64e7ba-e346-4660-820c-c3f4a28270b9",
   "metadata": {},
   "source": [
    "For semantic and syntactic evaluation, none of the models identified the missing term correctly for analogy-task questions. Due to this, another accuracy based on how the correct missing term is ranked based on similarity is used. Because of the vocabulary limitation, only a fraction of analogy-task questions for semantic and syntactic parts were used. Based on the summary results for semantic and syntactic, the correct missing term tends to be ranked at around the 50th percentile of all words in the vocabulary. For semantic accuracy, Skip-gram with negative sampling gave the best result while CBOW gave the best for syntactic accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5fdb91a-b938-4891-9cdb-aad391e3d5f3",
   "metadata": {},
   "source": [
    "From the results of the evaluation of the similarity dataset, the Spearman correlation coefficient tends to suggest that there is a weak correlation. But the p-value also tends to suggest that tends to be more uncorrelated. Due to these, none of the models gave the optimal performance. This is mainly due to the insufficient amount of data in the corpus the models were trained on vocabulary limitation as only 88 out of 203 sample points were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e472d873-b55a-40cc-bb0b-08692d04e0ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
