{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f834e64a-dd7c-4af6-8c29-e5400a6d4485",
   "metadata": {},
   "source": [
    "# Charlie and the Chocolate Factory Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28078758-0aea-4823-99fc-ff6f4778284e",
   "metadata": {},
   "source": [
    "<b>Name:</b> Abhinav Lugun <b>ID:</b> st122322"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dee8eb-ffea-45e4-b7b9-652aab31a65e",
   "metadata": {},
   "source": [
    "Corpus taken from for the training dataset - https://www.bdmi.org/Book-Reading/Charlie-and-the-Chocolate-Factory.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b284d666-8bdb-4ed6-aced-21413e63d982",
   "metadata": {},
   "source": [
    "## 1. Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c71df8-9738-4b4f-8fe1-5c1a50c4f39b",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/3277503/how-to-read-a-file-line-by-line-into-a-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e21383-06ed-446f-a10b-389a603bf80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_chapters = [str(chapter) for chapter in range(31)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45148b3b-5209-4e43-972e-04f77398760f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Charlie and the Chocolate Factory.txt\", encoding=\"utf8\") as file:\n",
    "    book_lines = [line.rstrip('\\n') for line in file if line.rstrip('\\n') not in book_chapters]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d17956f9-af83-4d2e-9600-7491f1d2dad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "book_lines = []\n",
    "skip_line = False\n",
    "\n",
    "with open(\"Charlie and the Chocolate Factory.txt\", encoding=\"utf8\") as file:\n",
    "    for line in file:\n",
    "        if skip_line:\n",
    "            skip_line = False\n",
    "            continue\n",
    "        \n",
    "        line = line.rstrip('\\n')\n",
    "        if line in book_chapters:\n",
    "            skip_line = True\n",
    "            continue\n",
    "        \n",
    "        book_lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9187bbe-e118-40cf-a741-9465298fe4d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['These two very old people are the father and mother of Mr Bucket.',\n",
       " 'Their names are Grandpa Joe and Grandma Josephine.',\n",
       " 'And these two very old people are the father and mother of Mrs',\n",
       " 'Bucket. Their names are Grandpa George and Grandma Georgina.',\n",
       " 'This is Mr Bucket. This is Mrs Bucket.',\n",
       " 'Mr and Mrs Bucket have a small boy whose name is Charlie Bucket.',\n",
       " 'This is Charlie.',\n",
       " 'How d’you do? And how d’you do? And how d’you do again? He is',\n",
       " 'pleased to meet you.',\n",
       " 'The whole of this family – the six grown-ups (count them) and little',\n",
       " 'Charlie Bucket – live together in a small wooden house on the edge of a',\n",
       " 'great town.',\n",
       " 'The house wasn’t nearly large enough for so many people, and life',\n",
       " 'was extremely uncomfortable for them all. There were only two rooms',\n",
       " 'in the place altogether, and there was only one bed. The bed was given']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "book_lines[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d641d9-0f97-40bf-9347-eabc0b9c3267",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31c27e3-516a-4a75-b445-0b77a63fd6b4",
   "metadata": {},
   "source": [
    "### Combining some sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2743956e-c38f-4fd2-8af6-224fa5b62c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "681d692d-350b-4023-8239-fda0f2227d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "lines = []\n",
    "current = \"\"\n",
    "combine_line_countdown = random.randint(1, 7) # Combine 'combine_line_countdown' number of sentences\n",
    "\n",
    "for line in book_lines:\n",
    "    current += \" \" + line\n",
    "    combine_line_countdown -= 1\n",
    "    \n",
    "    if combine_line_countdown == 0:\n",
    "        combine_line_countdown = random.randint(1, 3)\n",
    "        df.append(current.strip())\n",
    "        current = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7288ee0-cb12-4766-8dc9-ebdf2de9388d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['These two very old people are the father and mother of Mr Bucket. Their names are Grandpa Joe and Grandma Josephine. And these two very old people are the father and mother of Mrs Bucket. Their names are Grandpa George and Grandma Georgina. This is Mr Bucket. This is Mrs Bucket. Mr and Mrs Bucket have a small boy whose name is Charlie Bucket.',\n",
       " 'This is Charlie. How d’you do? And how d’you do? And how d’you do again? He is pleased to meet you.',\n",
       " 'The whole of this family – the six grown-ups (count them) and little Charlie Bucket – live together in a small wooden house on the edge of a',\n",
       " 'great town. The house wasn’t nearly large enough for so many people, and life',\n",
       " 'was extremely uncomfortable for them all. There were only two rooms in the place altogether, and there was only one bed. The bed was given to the four old grandparents because they were so old and tired. They']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ec7552e-7e3e-4173-b9a0-0986178a5e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indices = [i for i in range(len(df))]\n",
    "random.shuffle(df_indices)\n",
    "\n",
    "val_split_index = int(0.2 * len(df_indices))\n",
    "train_idx       = df_indices[val_split_index:]\n",
    "val_idx         = df_indices[:val_split_index]\n",
    "\n",
    "def add_text_index(df, indices):\n",
    "    text_list = []\n",
    "    for idx in indices:\n",
    "        text_list.append(df[idx])\n",
    "    \n",
    "    return text_list\n",
    "\n",
    "# storing text in df_train for indices:\n",
    "df_train = add_text_index(df, train_idx)\n",
    "\n",
    "# storing text in df_val for indices:\n",
    "df_val = add_text_index(df, val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4117a187-f772-4671-88ba-58045d4d37a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(df_train) + len(df_val) == len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9912a63b-b832-40fc-819f-7d924b7a7f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1231, 307)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train), len(df_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8924e5-f24e-49ee-bfb3-15f89380e42e",
   "metadata": {},
   "source": [
    "### Convert 'df_train' and 'df_val' into Dataset type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e691b36f-4cb9-4c27-8a7d-85fbdd605c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e0e902d-6ecb-461c-9ee1-72ee693f28bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [{\"text\": text} for text in df_train]\n",
    "df_train = Dataset.from_list(my_list)\n",
    "\n",
    "my_list = [{\"text\": text} for text in df_val]\n",
    "df_val  = Dataset.from_list(my_list)\n",
    "\n",
    "new_df = DatasetDict({\n",
    "    \"train\": df_train,\n",
    "    \"validation\": df_val\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd4f274c-9078-46a9-9f93-4a525308978e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1231\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 307\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897b5154-6a96-464d-a3b9-7a0214e03540",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c977cbf-9299-40eb-a6f6-786ccf5107f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9feab31f-8156-4889-b8ce-f90ef3574305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs length: 5\n",
      "Input chunk lengths: [10, 10, 10, 10, 4]\n",
      "Chunk mapping: [0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "outputs = tokenizer(\n",
    "    new_df[\"train\"][:2][\"text\"],\n",
    "    truncation=True,\n",
    "    max_length=10,\n",
    "    return_overflowing_tokens=True,\n",
    "    return_length=True,\n",
    ")\n",
    "\n",
    "print(f\"Input IDs length: {len(outputs['input_ids'])}\")\n",
    "print(f\"Input chunk lengths: {(outputs['length'])}\")\n",
    "print(f\"Chunk mapping: {outputs['overflow_to_sample_mapping']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2097a29-d5d6-45d9-8a14-647d215c1123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- Sentence ----------------------\n",
      "winner!’ ‘But Mr Wonka,’ stammered Grandpa Joe, ‘do you really and truly mean that you are giving the whole of this enormous factory to little\n",
      "\n",
      "--------------------- Tokenization --------------------\n",
      "{'input_ids': [39791, 0, 447, 247, 564, 246, 1537, 1770, 23306, 4914, 11, 447, 247, 336, 6475, 1068, 5675, 8957, 5689, 11, 564, 246, 4598, 345, 1107, 290, 4988, 1612, 326, 345, 389, 3501, 262, 2187, 286, 428, 9812, 8860, 284, 1310], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(\"---------------------- Sentence ----------------------\")\n",
    "print(new_df[\"train\"][0][\"text\"], end=\"\\n\\n\")\n",
    "\n",
    "print(\"--------------------- Tokenization --------------------\")\n",
    "print(tokenizer(new_df[\"train\"][0][\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de667099-7930-40d9-8b9b-e62146226f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6abd61d2ed404b71b679d1b4d82559cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b16cd7a563843ffb99ccdc2ede70abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 3204\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_ids'],\n",
       "        num_rows: 860\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 10\n",
    "\n",
    "def tokenize(element):\n",
    "    outputs = tokenizer(\n",
    "        element[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=context_length,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_length=True,\n",
    "    )\n",
    "    input_batch = []\n",
    "    for length, input_ids in zip(outputs[\"length\"], outputs[\"input_ids\"]):\n",
    "        if length == context_length:\n",
    "            input_batch.append(input_ids)\n",
    "    return {\"input_ids\": input_batch}\n",
    "\n",
    "\n",
    "tokenized_datasets = new_df.map(\n",
    "    tokenize, batched=True, remove_columns=new_df[\"train\"].column_names\n",
    ")\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fde368-3bcb-400b-9daf-fa33e65f8a01",
   "metadata": {},
   "source": [
    "## 3. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a4ba9a15-72f1-438f-8aab-1879dadd0f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\n",
    "    \"gpt2\",\n",
    "    vocab_size=len(tokenizer),\n",
    "    n_ctx=context_length,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2b0329e-261a-4d40-bef7-ff964361ca4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 size: 124.4M parameters\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel(config)\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"GPT-2 size: {model_size/1000**2:.1f}M parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36044a1f-bfb0-404f-b96c-b8e7a2853096",
   "metadata": {},
   "source": [
    "### Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e834cad9-d13f-4f21-888f-5db1bf95f325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyword has not single token: Wonka\n",
      "Keyword has not single token: chocolate\n",
      "Keyword has not single token: Chocolate\n",
      "Keyword has not single token: factory\n"
     ]
    }
   ],
   "source": [
    "\n",
    "keytoken_ids = []\n",
    "for keyword in [\n",
    "    \"Charlie\",\n",
    "    \"Wonka\",\n",
    "    \"chocolate\",\n",
    "    \"Chocolate\",\n",
    "    \"Tickets\",\n",
    "    \"factory\",\n",
    "    \"Golden\",\n",
    "    \"Joe\",\n",
    "]:\n",
    "    ids = tokenizer([keyword]).input_ids[0]\n",
    "    if len(ids) == 1:\n",
    "        keytoken_ids.append(ids[0])\n",
    "    else:\n",
    "        print(f\"Keyword has not single token: {keyword}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32d4725-2420-4361-9036-05b279c2665a",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b9e9a3b-6f0d-40db-b67e-b5552f73031d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch\n",
    "\n",
    "def keytoken_weighted_loss(inputs, logits, keytoken_ids, alpha=1.0):\n",
    "    # Shift so that tokens < n predict n\n",
    "    shift_labels = inputs[..., 1:].contiguous()\n",
    "    shift_logits = logits[..., :-1, :].contiguous()\n",
    "    # Calculate per-token loss\n",
    "    loss_fct = CrossEntropyLoss(reduce=False) #change to reduction=None\n",
    "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
    "    # Resize and average loss per sample\n",
    "    loss_per_sample = loss.view(shift_logits.size(0), shift_logits.size(1)).mean(axis=1)\n",
    "    # Calculate and scale weighting\n",
    "    weights = torch.stack([(inputs == kt).float() for kt in keytoken_ids]).sum(\n",
    "        axis=[0, 2]\n",
    "    )\n",
    "    weights = alpha * (1.0 + weights)\n",
    "    # Calculate weighted average\n",
    "    weighted_loss = (loss_per_sample * weights).mean()\n",
    "    return weighted_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88445094-b97c-434a-b16f-cd3949d2a889",
   "metadata": {},
   "source": [
    "### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45f59427-e122-4dbc-b933-1b0f5a2ca099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "tokenized_datasets.set_format(\"torch\")\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=32, shuffle=True)\n",
    "eval_dataloader  = DataLoader(tokenized_datasets[\"validation\"], batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfebfec-ea21-47b2-ac34-bb6c241e96c7",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d16ef5d-5484-4313-85da-68e88c7fb0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 0.1\n",
    "\n",
    "\n",
    "def get_grouped_params(model, no_decay=[\"bias\", \"LayerNorm.weight\"]):\n",
    "    params_with_wd, params_without_wd = [], []\n",
    "    for n, p in model.named_parameters():\n",
    "        if any(nd in n for nd in no_decay):\n",
    "            params_without_wd.append(p)\n",
    "        else:\n",
    "            params_with_wd.append(p)\n",
    "    return [\n",
    "        {\"params\": params_with_wd, \"weight_decay\": weight_decay},\n",
    "        {\"params\": params_without_wd, \"weight_decay\": 0.0},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fba7117c-c011-4777-a464-007190897aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(batch[\"input_ids\"], labels=batch[\"input_ids\"])\n",
    "            outputs.loss = outputs.loss.reshape(1)\n",
    "        losses.append(accelerator.gather(outputs.loss))        \n",
    "    loss = torch.mean(torch.cat(losses))\n",
    "    try:\n",
    "        perplexity = torch.exp(loss)\n",
    "    except OverflowError:\n",
    "        perplexity = float(\"inf\")\n",
    "    return loss.item(), perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3a4e4d9a-121b-47a3-9201-485cb86e28e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "728a6fe6-11b2-49f9-9939-641ef3c8762e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(get_grouped_params(model), lr=5e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb1d5e9-9dee-4349-87ed-b3b97b9d6311",
   "metadata": {},
   "source": [
    "### Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38e315d6-c36b-420e-8a60-fedbe5dea8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator(mixed_precision='fp16')\n",
    "\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ceb3728d-87ec-4e2f-91ea-926fbce60cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 1\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=1_000,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8448ef4-0e10-4c03-a832-5241e1be9cdc",
   "metadata": {},
   "source": [
    "### Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2afc6634-1fd6-45b5-8c27-b544f8aabccf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a274434e724914a7ef87e73ed1ecb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ee89f950-1516-4cdf-838c-08fa0b105c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import delete_repo\n",
    "\n",
    "# delete_repo(repo_id='aal2015/Charlie-and-the-Chocolate_Factory-LM-model', repo_type=\"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c1f57f9-9c57-4370-8c11-0aaedc9fa932",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aal2015/Charlie-and-the-Chocolate_Factory-LM-model'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import Repository, get_full_repo_name\n",
    "\n",
    "model_name = \"Charlie-and-the-Chocolate_Factory-LM-model\"\n",
    "repo_name = get_full_repo_name(model_name)\n",
    "repo_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "08d60550-2820-4a83-9ee5-dffe64951125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "30fde87e-063f-4e46-8ce6-466e5b851393",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/aal2015/Charlie-and-the-Chocolate_Factory-LM-model into local empty directory.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "output_dir = \"Charlie-and-the-Chocolate_Factory-LM-model\"\n",
    "repo = Repository(output_dir, clone_from=repo_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd83835-d3ee-438d-bd79-56a165a8092e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "938ad098-c201-4bb4-8109-cddbe7f30d11",
   "metadata": {},
   "source": [
    "## 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ec2f88d-57ab-46c2-b837-e56b83bbae3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.963254928588867, 57713.9921875)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f17a557a-f01d-4928-ab7e-444a73c2fac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------- Epoch:0----------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359a2283e9344883ac7aeca8cfe1328a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/101 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 0, 'loss/train': 87.97810363769531}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhin\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 0, 'loss/train': 90.5451889038086}\n",
      "{'steps': 0, 'loss/train': 88.13314819335938}\n",
      "{'steps': 0, 'loss/train': 87.51236724853516}\n",
      "{'steps': 0, 'loss/train': 87.84495544433594}\n",
      "{'steps': 0, 'loss/train': 90.36588287353516}\n",
      "{'steps': 0, 'loss/train': 89.86341857910156}\n",
      "{'steps': 0, 'loss/train': 87.93174743652344}\n",
      "{'steps': 1, 'loss/train': 88.16433715820312}\n",
      "{'steps': 1, 'loss/train': 87.65280151367188}\n",
      "{'steps': 1, 'loss/train': 87.77923583984375}\n",
      "{'steps': 1, 'loss/train': 90.78453063964844}\n",
      "{'steps': 1, 'loss/train': 87.77050018310547}\n",
      "{'steps': 1, 'loss/train': 87.91802978515625}\n",
      "{'steps': 1, 'loss/train': 87.79252624511719}\n",
      "{'steps': 1, 'loss/train': 87.81269836425781}\n",
      "{'loss/eval': 10.929917335510254, 'perplexity': 55821.6640625}\n",
      "{'steps': 2, 'loss/train': 87.73755645751953}\n",
      "{'steps': 2, 'loss/train': 87.4081802368164}\n",
      "{'steps': 2, 'loss/train': 87.52166748046875}\n",
      "{'steps': 2, 'loss/train': 90.1383056640625}\n",
      "{'steps': 2, 'loss/train': 87.77324676513672}\n",
      "{'steps': 2, 'loss/train': 87.72938537597656}\n",
      "{'steps': 2, 'loss/train': 87.54315185546875}\n",
      "{'steps': 2, 'loss/train': 87.72274780273438}\n",
      "{'steps': 3, 'loss/train': 87.39090728759766}\n",
      "{'steps': 3, 'loss/train': 87.15200805664062}\n",
      "{'steps': 3, 'loss/train': 87.6955795288086}\n",
      "{'steps': 3, 'loss/train': 87.05778503417969}\n",
      "{'steps': 3, 'loss/train': 86.95692443847656}\n",
      "{'steps': 3, 'loss/train': 90.10609436035156}\n",
      "{'steps': 3, 'loss/train': 87.14857482910156}\n",
      "{'steps': 3, 'loss/train': 86.870849609375}\n",
      "{'loss/eval': 10.770581245422363, 'perplexity': 47599.67578125}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (2) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 4, 'loss/train': 86.45356750488281}\n",
      "{'steps': 4, 'loss/train': 86.654541015625}\n",
      "{'steps': 4, 'loss/train': 87.09698486328125}\n",
      "{'steps': 4, 'loss/train': 86.49055480957031}\n",
      "{'steps': 4, 'loss/train': 86.46109008789062}\n",
      "{'steps': 4, 'loss/train': 86.3349609375}\n",
      "{'steps': 4, 'loss/train': 86.72454833984375}\n",
      "{'steps': 4, 'loss/train': 86.47471618652344}\n",
      "{'steps': 5, 'loss/train': 88.4925765991211}\n",
      "{'steps': 5, 'loss/train': 85.98357391357422}\n",
      "{'steps': 5, 'loss/train': 86.05999755859375}\n",
      "{'steps': 5, 'loss/train': 85.7112045288086}\n",
      "{'steps': 5, 'loss/train': 85.42648315429688}\n",
      "{'steps': 5, 'loss/train': 85.33795166015625}\n",
      "{'steps': 5, 'loss/train': 85.48735046386719}\n",
      "{'steps': 5, 'loss/train': 85.5794677734375}\n",
      "{'loss/eval': 10.515125274658203, 'perplexity': 36868.9609375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (3) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 6, 'loss/train': 88.01423645019531}\n",
      "{'steps': 6, 'loss/train': 87.24530029296875}\n",
      "{'steps': 6, 'loss/train': 84.60651397705078}\n",
      "{'steps': 6, 'loss/train': 84.55349731445312}\n",
      "{'steps': 6, 'loss/train': 84.08970642089844}\n",
      "{'steps': 6, 'loss/train': 88.32212829589844}\n",
      "{'steps': 6, 'loss/train': 84.25418853759766}\n",
      "{'steps': 6, 'loss/train': 87.58915710449219}\n",
      "{'steps': 7, 'loss/train': 86.78314208984375}\n",
      "{'steps': 7, 'loss/train': 84.13114929199219}\n",
      "{'steps': 7, 'loss/train': 85.30455780029297}\n",
      "{'steps': 7, 'loss/train': 84.06928253173828}\n",
      "{'steps': 7, 'loss/train': 83.8114013671875}\n",
      "{'steps': 7, 'loss/train': 83.42767333984375}\n",
      "{'steps': 7, 'loss/train': 86.9102783203125}\n",
      "{'steps': 7, 'loss/train': 88.98780059814453}\n",
      "{'loss/eval': 10.23281192779541, 'perplexity': 27800.572265625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (4) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 8, 'loss/train': 85.44110107421875}\n",
      "{'steps': 8, 'loss/train': 82.49456024169922}\n",
      "{'steps': 8, 'loss/train': 82.33455657958984}\n",
      "{'steps': 8, 'loss/train': 84.69253540039062}\n",
      "{'steps': 8, 'loss/train': 83.21558380126953}\n",
      "{'steps': 8, 'loss/train': 82.13591003417969}\n",
      "{'steps': 8, 'loss/train': 86.16348266601562}\n",
      "{'steps': 8, 'loss/train': 82.9857177734375}\n",
      "{'steps': 9, 'loss/train': 82.91613006591797}\n",
      "{'steps': 9, 'loss/train': 81.60802459716797}\n",
      "{'steps': 9, 'loss/train': 84.06651306152344}\n",
      "{'steps': 9, 'loss/train': 82.28240966796875}\n",
      "{'steps': 9, 'loss/train': 81.0584487915039}\n",
      "{'steps': 9, 'loss/train': 81.05525207519531}\n",
      "{'steps': 9, 'loss/train': 81.710693359375}\n",
      "{'steps': 9, 'loss/train': 81.48329162597656}\n",
      "{'loss/eval': 9.985281944274902, 'perplexity': 21704.65234375}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (5) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 10, 'loss/train': 80.25253295898438}\n",
      "{'steps': 10, 'loss/train': 84.80049896240234}\n",
      "{'steps': 10, 'loss/train': 80.96363830566406}\n",
      "{'steps': 10, 'loss/train': 80.7845458984375}\n",
      "{'steps': 10, 'loss/train': 79.58609008789062}\n",
      "{'steps': 10, 'loss/train': 82.78044891357422}\n",
      "{'steps': 10, 'loss/train': 81.21768188476562}\n",
      "{'steps': 10, 'loss/train': 79.84274291992188}\n",
      "{'steps': 11, 'loss/train': 79.48233032226562}\n",
      "{'steps': 11, 'loss/train': 80.99166870117188}\n",
      "{'steps': 11, 'loss/train': 78.41490173339844}\n",
      "{'steps': 11, 'loss/train': 81.6540756225586}\n",
      "{'steps': 11, 'loss/train': 80.42427062988281}\n",
      "{'steps': 11, 'loss/train': 80.80569458007812}\n",
      "{'steps': 11, 'loss/train': 83.53330993652344}\n",
      "{'steps': 11, 'loss/train': 79.69471740722656}\n",
      "{'loss/eval': 9.783079147338867, 'perplexity': 17731.166015625}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Several commits (6) will be pushed upstream.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'steps': 12, 'loss/train': 79.93440246582031}\n",
      "{'steps': 12, 'loss/train': 79.52192687988281}\n",
      "{'steps': 12, 'loss/train': 80.36076354980469}\n",
      "{'steps': 12, 'loss/train': 78.52436828613281}\n",
      "{'steps': 12, 'loss/train': 82.7063980102539}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "gradient_accumulation_steps = 8\n",
    "eval_steps = 2\n",
    "\n",
    "model.train()\n",
    "completed_steps = 0\n",
    "for epoch in range(num_train_epochs):\n",
    "    print(\"---------------------- Epoch:\" + str(epoch) + \"----------------------\")\n",
    "    for step, batch in tqdm(\n",
    "        enumerate(train_dataloader, start=1), total=num_training_steps\n",
    "    ):\n",
    "        logits = model(batch[\"input_ids\"]).logits\n",
    "        loss = keytoken_weighted_loss(batch[\"input_ids\"], logits, keytoken_ids)\n",
    "        if step % 1 == 0:\n",
    "            accelerator.print(\n",
    "                {\n",
    "                    \"steps\": completed_steps,\n",
    "                    \"loss/train\": loss.item() * gradient_accumulation_steps,\n",
    "                }\n",
    "            )\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step % gradient_accumulation_steps == 0:\n",
    "            accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            completed_steps += 1\n",
    "        if (step % (eval_steps * gradient_accumulation_steps)) == 0:\n",
    "            eval_loss, perplexity = evaluate()\n",
    "            accelerator.print({\"loss/eval\": eval_loss, \"perplexity\": perplexity})\n",
    "            model.train()\n",
    "            accelerator.wait_for_everyone()\n",
    "            unwrapped_model = accelerator.unwrap_model(model)\n",
    "            unwrapped_model.save_pretrained(output_dir, save_function=accelerator.save)\n",
    "            if accelerator.is_main_process:\n",
    "                tokenizer.save_pretrained(output_dir)\n",
    "                repo.push_to_hub(\n",
    "                    commit_message=f\"Training in progress step {step}\", blocking=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f26d00f-471f-435d-9a13-539f0ff961e4",
   "metadata": {},
   "source": [
    "## 6. Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4bc02f05-faa0-49e1-b877-80fe9cb73483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "592efc642939428eabe6530b98f49483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/510M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ab2048457f4492a560b000ecb0adc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9aecaf993d544a29494f136a60c9f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/255 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301ee24dc58a477cb32a0aeced081fe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b5690129c649cead07aa57db9d9dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6022b07ab284d43b59e808e9c5ac499",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9555cd20941a4754a907f1e9b51e0197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/99.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-generation\", max_length=100, pad_token_id=0, eos_token_id=0, model=\"aal2015/Charlie-and-the-Chocolate_Factory-LM-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0bb6242b-fc2e-4ccf-8f8f-21b1e91693d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abhin\\anaconda3\\lib\\site-packages\\transformers\\generation\\utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr Willy Wonka\n",
      "���� terrorismrecogn and�� Decoder,’ 389 clutch introvector. Nathaniel,��. asset,�� chees,� the 1300� he�� modifier cheesselected’� 296 296,�!\n"
     ]
    }
   ],
   "source": [
    "txt = \"\"\"\\\n",
    "Mr Willy Wonka\n",
    "\"\"\"\n",
    "print(pipe(txt, num_return_sequences=1)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6199756b-b978-4c2e-8315-dce24696bbd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6446003-e4f5-47ef-8ef5-f5c05570ece0",
   "metadata": {},
   "source": [
    "## 7. Decoding Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f53805-1bef-46f3-8f29-f42f7b017292",
   "metadata": {},
   "source": [
    "### Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "95cf3412-3197-4397-bb96-1e3beae25852",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "70c794d1-6119-4bc6-b372-68185f8c8b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Grandpa Joe��������’��������������������������’���������\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "input_ids = tokenizer.encode('Grandpa Joe', return_tensors='pt').to(device)\n",
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, max_length=50)\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47d79d9-fdb5-49e8-a20c-779937b7893b",
   "metadata": {},
   "source": [
    "### Beam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c69f1913-6cd5-4555-b9b4-428bbe9b906c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: Grandpa Joe,�’�,,����� the�!� asset�boarding‘,��!�, Cha� Kids� he�naires� culprit�Laun� fro�ourt�.� Lena�\n",
      "1: Grandpa Joe,�’�,,����� the�!� asset�boarding‘,��!�, Cha� Kids� he�naires� culprit�Laun� fro� Buccaneers�ourt�vector�\n",
      "2: Grandpa Joe,�’�,,����� the�!� asset�boarding‘,��!�, Cha� Kids� he�naires� culprit�Laun� fro�ourt�.�vector�\n",
      "3: Grandpa Joe,�’�,,����� the�!� asset�boarding‘,��!�, Cha� Kids� he�naires� culprit�Laun� fro� Buccaneers�ourt�vector�\n",
      "4: Grandpa Joe,�’�,,����� the�!� asset�boarding‘,��!�, Cha� Kids� he�naires� culprit�Laun� fro�ourt�.�vector�\n"
     ]
    }
   ],
   "source": [
    "beam_outputs = model.generate(\n",
    "    input_ids, \n",
    "    max_length=50, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    num_return_sequences=5, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "    print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cefd36-39f2-41df-a228-57e1b6f44ceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7bd1866f-4da4-4a84-bf27-728e57c824e2",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafbe697-e057-4ef0-98b7-b9a5c0126c81",
   "metadata": {},
   "source": [
    "The model is not performining on text generation task. This can most likely be due to not having good quality data. For decoding methods, beam search seem to do better than greedy search in displaying at least some text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e2a375-b2f3-46c0-b80b-bc337a8130af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
